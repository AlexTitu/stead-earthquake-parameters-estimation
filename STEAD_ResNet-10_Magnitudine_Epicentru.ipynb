{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8k8xwh83ZJK",
        "outputId": "17d209c2-6e9e-41b6-ce55-c9e072175783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtnt>=0.0.5 (from torcheval)\n",
            "  Downloading torchtnt-0.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (5.9.5)\n",
            "Collecting pyre-extensions (from torchtnt>=0.0.5->torcheval)\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
            "Collecting typing-inspect (from pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (16.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, torchtnt, torcheval\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.1.0 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEzsjsCZ3OaN",
        "outputId": "abb34df6-6088-46d0-d322-9fe4ab6b24f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 6000 samples for each waveform recorded for 60 seconds of ground motion - sampling rate 100Hz\n",
        "# 41365 waveforms - size of dataset for the chosen device PB;B082;33.598182;-116.596005;1374.8 HH & EH\n",
        "# Magnitude Type - ml (local) - ~2.0 to ~6.5 magnitudes - 0 - 600 km distance range ;\n",
        "# source latitude + longitude = epicenter + depth = hypocenter\n",
        "\n",
        "# 4 grafice, pentru fiecare loss pentru train si validare (done) + checkpoint\n",
        "# pe test pe train si val - un tabel care sa contina toate marimile + R2 pe test\n",
        "\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import numpy as np\n",
        "import librosa as lib\n",
        "import librosa.display as libd\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "plots_dir = '/content/drive/My Drive/Plots/magn_epicenter'\n",
        "models_dir = '/content/drive/My Drive/Models/magn_epicenter'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-70OKCLZ6C2Y"
      },
      "outputs": [],
      "source": [
        "class STEADDataset(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file,hdf5_file,transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        hdf5_file (string): File with all the waveforms.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.tags = pd.read_csv(csv_file)\n",
        "    # self.tags = self.tags [(self.tags.trace_category == 'earthquake_local') & (self.tags.source_distance_km<=20)&(self.tags.source_magnitude > 3)]\n",
        "    self.hdf5_file = hdf5_file\n",
        "    self.traces = self.tags['trace_name'].to_list()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.traces)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dataset = h5py.File(self.hdf5_file,'r')\n",
        "    tracename = self.traces[idx]\n",
        "    waveform = dataset.get('data/'+tracename)\n",
        "    data = np.array(waveform)\n",
        "    spectrograms = self.getSpectrogram(data)\n",
        "\n",
        "    sample = {'spectrograms':spectrograms, 'source_magnitude':waveform.attrs['source_magnitude'],\n",
        "              'source_latitude':waveform.attrs['source_latitude'],'source_longitude':waveform.attrs['source_longitude']}\n",
        "\n",
        "    if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "    dataset.close()\n",
        "    return sample\n",
        "\n",
        "  def getSpectrogram(self, waveforms):\n",
        "    # defining axis\n",
        "    EW = waveforms[:,0]\n",
        "    NS = waveforms[:,1]\n",
        "    Vert = waveforms[:,2]\n",
        "\n",
        "    EW_ft = lib.stft(EW,n_fft=1024, hop_length=16) # n_fft = dimensiunea semnalului din fereastra stft = initial 2048\n",
        "    NS_ft = lib.stft(NS,n_fft=1024, hop_length=16)\n",
        "    Vert_ft = lib.stft(Vert,n_fft=1024, hop_length=16) # window = 'hann' # 1024 sau 512\n",
        "    EW_db = lib.amplitude_to_db(np.abs(EW_ft), ref=np.max) # np.max = normare\n",
        "    NS_db = lib.amplitude_to_db(np.abs(NS_ft), ref=np.max)\n",
        "    Vert_db = lib.amplitude_to_db(np.abs(Vert_ft), ref=np.max)\n",
        "\n",
        "    spectrograms = np.array([EW_db, NS_db, Vert_db])\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        spectrograms, results = sample['spectrograms'], np.array([sample['source_magnitude'],sample['source_latitude'],sample['source_longitude']], dtype=np.float32)\n",
        "\n",
        "        return {'spectrograms': torch.from_numpy(spectrograms),\n",
        "                'results':  torch.from_numpy(results)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VIF677lV3bK",
        "outputId": "4d030e11-cbd5-4484-8734-b2dbe7c84a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-772ffaec51f1>:10: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.tags = pd.read_csv(csv_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3])\n",
            "torch.Size([3, 513, 376])\n",
            "torch.Size([3])\n",
            "torch.Size([3, 513, 376])\n",
            "torch.Size([3])\n",
            "torch.Size([3, 513, 376])\n",
            "torch.Size([3])\n",
            "torch.Size([3, 513, 376])\n",
            "41365\n"
          ]
        }
      ],
      "source": [
        "STEAD_dataset = STEADDataset(csv_file=\"drive/My Drive/dataset.csv\",\n",
        "                             hdf5_file=\"drive/My Drive/dataset.hdf5\",\n",
        "                             transform=ToTensor())\n",
        "\n",
        "for i in range(len(STEAD_dataset)):\n",
        "  sample = STEAD_dataset[i]\n",
        "  print(sample['results'].size())\n",
        "  print(sample['spectrograms'].size())\n",
        "  if i == 3:\n",
        "    break\n",
        "\n",
        "print(len(STEAD_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y46scF2YAp65"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module # implement a class rather than using Sequential object\n",
        "from torch.nn import Conv2d # convolutional layer\n",
        "from torch.nn import BatchNorm2d #BatchNormalization\n",
        "from torch.nn import Linear # Fully connected layers\n",
        "from torch.nn import MaxPool2d # 2D max-pooling to reduce spatial dimensions\n",
        "from torch.nn import ReLU # activation function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Scheduler that reduces learning rate on plateau\n",
        "from torch import flatten # Flattens the output of a multidimensional volume (CONV or POOl layer) -> Fully connected layer\n",
        "from torch.utils.data import random_split # for splitting Dataset into Train, Evaluation and Test\n",
        "from torcheval.metrics import R2Score # Evaluates model's accuracy\n",
        "from torch import nn\n",
        "import torch\n",
        "import time # for timing train loop\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFp058dTkagm"
      },
      "outputs": [],
      "source": [
        "# defining residual block class\n",
        "class block(Module):\n",
        "  # identity_downsample = conv layers which we might need to do depending on if we ve changed input size or channel numbers\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(5,5), padding=(2,2)):\n",
        "    super(block, self).__init__()\n",
        "    self.conv1 = Conv2d(in_channels, out_channels, kernel_size= kernel_size, padding=padding)\n",
        "    self.bn1 = BatchNorm2d(out_channels)\n",
        "    self.conv2 = Conv2d(out_channels, out_channels, kernel_size= kernel_size, padding=padding)\n",
        "    self.bn2 = BatchNorm2d(out_channels)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    identity = x\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x += identity\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    output = self.maxpool(x)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class EqNet(Module):\n",
        "  def __init__(self, block, numChannels, outputNodes):\n",
        "    super(EqNet, self).__init__()\n",
        "    # initialize first CONV => RELU => POOL layer\n",
        "    self.conv1 = Conv2d(in_channels=numChannels, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.bn1 = BatchNorm2d(16)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize second CONV => RELU => POOL layer\n",
        "    self.conv2 = Conv2d(in_channels=16, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.bn2 = BatchNorm2d(16)\n",
        "\n",
        "    # ResNet layers\n",
        "    self.layer1 = block(in_channels=16, out_channels=32, kernel_size=(5,5), padding=(2,2))\n",
        "    self.layer2 = block(in_channels=32, out_channels=64, kernel_size=(5,5), padding=(2,2))\n",
        "    self.layer3 = block(in_channels=64, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "    self.layer4 = block(in_channels=96, out_channels=128, kernel_size=(3,3), padding=(1,1))\n",
        "\n",
        "    # initialize last CONV => RELU => CONV => POOL layer\n",
        "    self.conv3 = Conv2d(in_channels=128, out_channels=256, kernel_size=(3,3), padding=(1,1))\n",
        "    self.bn3 = BatchNorm2d(256)\n",
        "    self.conv4 = Conv2d(in_channels=256, out_channels=256, kernel_size=(3,3), padding=(1,1))\n",
        "    self.bn4 = BatchNorm2d(256)\n",
        "\n",
        "    # initialize first (and only) set of FC => ReLU layers - fully connected layer\n",
        "    self.fc1 = Linear(in_features=2048, out_features=1024)\n",
        "\n",
        "    # initialize first (and only) set of FC => Linear *Regression* Layers\n",
        "    self.fc2 = Linear(in_features=1024, out_features=outputNodes)\n",
        "\n",
        "  def forward(self, x):\n",
        "  # pass the input through the first set of CONV -> ReLU -> POOL layers\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # pass the input through the second set of CONV -> ReLU -> POOL layer\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # pass the input through resnet\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    # pass the input through the last CONV => RELU => CONV => POOL layer\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # flatten the output from the previous layer and pass it through FC layer\n",
        "    x = flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass the output to our Linear layer for regression predictions\n",
        "    output = self.fc2(x)\n",
        "\n",
        "    # return the output predictions\n",
        "    return output\n",
        "\n",
        "# defining Early Stopping class\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience = 1, min_delta = 0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation_loss = np.inf\n",
        "    self.best_epoch = 0\n",
        "    self.best_train = [None] * 4\n",
        "    self.best_val = [None] * 4\n",
        "\n",
        "  def earlyStop(self, validation_loss, epoch, TrainLoss, ValLoss):\n",
        "    if validation_loss <= self.min_validation_loss:\n",
        "      print(\"[INFO] In EPOCH {} the loss value improved from {:.5f} to {:.5f}\".format(epoch, self.min_validation_loss, validation_loss))\n",
        "      self.min_validation_loss = validation_loss\n",
        "      self.counter = 0\n",
        "      self.best_epoch = epoch\n",
        "      torch.save(model.state_dict(), f\"{models_dir}/EqNet_state_dict.pt\")\n",
        "      self.setBestLosses(TrainLoss, ValLoss)\n",
        "\n",
        "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "      self.counter += 1\n",
        "      print(\"[INFO] In EPOCH {} the loss value did not improve from {:.5f}. This is the {} EPOCH in a row.\".format(epoch, self.min_validation_loss, self.counter))\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def setCounter(self, counter_state):\n",
        "    self.counter = counter_state\n",
        "\n",
        "  def setMinValLoss(self, ValLoss):\n",
        "    self.min_validation_loss = ValLoss\n",
        "\n",
        "  def setBestLosses(self, TrainLoss, ValLoss):\n",
        "    self.best_train = TrainLoss\n",
        "    self.best_val = ValLoss\n",
        "\n",
        "  def getBestTrainLosses(self):\n",
        "    return self.best_train\n",
        "\n",
        "  def getBestValLosses(self):\n",
        "    return self.best_val\n",
        "\n",
        "  def getBestEpoch(self):\n",
        "    return self.best_epoch\n",
        "\n",
        "  def saveLossesLocally(self):\n",
        "    np.save(f'{models_dir}/losses_train_reSnet.npy', np.array(self.best_train))\n",
        "    np.save(f'{models_dir}/losses_val_reSnet.npy', np.array(self.best_val))\n",
        "\n",
        "  def loadLossesLocally(self):\n",
        "    self.best_train = np.load(f'{models_dir}/losses_train_reSnet.npy')\n",
        "    self.best_val = np.load(f'{models_dir}/losses_val_reSnet.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaYTXUM_Ejs",
        "outputId": "573e95da-0054-4886-9552-994d6ad988de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device used for training...cuda\n"
          ]
        }
      ],
      "source": [
        "# define training hyperparameters\n",
        "INIT_LR = 1*1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# define the train and validation splits\n",
        "TRAIN_SPLIT = 0.70\n",
        "VAL_TEST_SPLIT = 0.15\n",
        "\n",
        "# set the device we will be using to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] device used for training...{}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQSr35LAfSk",
        "outputId": "a18499d2-83ba-4ecc-a37b-b767f47e3358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] generating the train/validation split...\n"
          ]
        }
      ],
      "source": [
        "# calculate the train/validation split\n",
        "print(\"[INFO] generating the train/validation split...\")\n",
        "numTrainSamples = int(len(STEAD_dataset)*TRAIN_SPLIT)\n",
        "numValSamples = int(len(STEAD_dataset)*VAL_TEST_SPLIT)\n",
        "numTestSamples = int(len(STEAD_dataset)-(numTrainSamples+numValSamples))\n",
        "\n",
        "(trainData, valData, testData) = random_split(STEAD_dataset,[numTrainSamples, numValSamples, numTestSamples], generator=torch.Generator().manual_seed(19))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpR-V_0qNCUW"
      },
      "outputs": [],
      "source": [
        "# initialize the train, validation and test dataloaders\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE)\n",
        "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE)\n",
        "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE)\n",
        "\n",
        "# calculate steps per epoch for training, validation set\n",
        "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
        "valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
        "testSteps = len(testDataLoader.dataset) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "espa39G3PHxX",
        "outputId": "e7810617-4b41-4f29-a24a-0d649ba147d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] initializing the EqNet model...\n",
            "[INFO] training the network...\n"
          ]
        }
      ],
      "source": [
        "# load from drive previous results\n",
        "previous_state = torch.load(f\"{models_dir}/reSnet_train_state_dict.pt\")\n",
        "\n",
        "# initialize the EqNet model\n",
        "print(\"[INFO] initializing the EqNet model...\")\n",
        "model = EqNet(block=block, numChannels=3, outputNodes=3).to(device)\n",
        "model.load_state_dict(previous_state['model_state_dict'])\n",
        "\n",
        "# initialize the optimizer and lr_scheduler\n",
        "opt = Adam(model.parameters(), lr=INIT_LR)\n",
        "opt.load_state_dict(previous_state['optimizer_state_dict'])\n",
        "\n",
        "scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.3, patience=2)\n",
        "scheduler.load_state_dict(previous_state['scheduler_state_dict'])\n",
        "\n",
        "# initialize loss function and accuracy measurement\n",
        "lossFn = nn.MSELoss()\n",
        "r2score_metric = R2Score(multioutput='raw_values', device=device)\n",
        "\n",
        "# initialize a dictionary to store training and evaluation history\n",
        "H = {\n",
        "    \"magnitude_train_loss\":[],\n",
        "    \"latitude_train_loss\":[],\n",
        "    \"longitude_train_loss\":[],\n",
        "    \"magnitude_val_loss\":[],\n",
        "    \"latitude_val_loss\":[],\n",
        "    \"longitude_val_loss\":[]\n",
        "}\n",
        "\n",
        "# load previous loss history and last epoch\n",
        "H = previous_state['train_loss_history']\n",
        "last_epoch = previous_state['epoch']+1\n",
        "\n",
        "# measure how long training is going to take\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Bcz3bGOU-En",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc2ee7f-49a2-4254-9de1-eb03f91fc733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EPOCH: 8/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.04285, 0.04939 , 0.02986 , 0.04931\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.05300, 0.03146 , 0.02174 , 0.10581\n",
            "[INFO] In EPOCH 8 the loss value did not improve from 0.04710. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 9/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.02163, 0.03011 , 0.01570 , 0.01907\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.02079, 0.02312 , 0.01794 , 0.02132\n",
            "[INFO] In EPOCH 9 the loss value improved from 0.04710 to 0.02079\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 10/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.02138, 0.02921 , 0.01521 , 0.01972\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.12938, 0.02471 , 0.06349 , 0.29993\n",
            "[INFO] In EPOCH 10 the loss value did not improve from 0.02079. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 11/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.02175, 0.02804 , 0.01497 , 0.02224\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.03376, 0.02359 , 0.02135 , 0.05634\n",
            "[INFO] In EPOCH 11 the loss value did not improve from 0.02079. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 12/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.02151, 0.02836 , 0.01367 , 0.02252\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.05347, 0.04671 , 0.02218 , 0.09151\n",
            "[INFO] In EPOCH 12 the loss value did not improve from 0.02079. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 13/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01454, 0.02286 , 0.00934 , 0.01143\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.01942, 0.02231 , 0.01033 , 0.02564\n",
            "[INFO] In EPOCH 13 the loss value improved from 0.02079 to 0.01942\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 14/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01421, 0.02265 , 0.00897 , 0.01100\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.01475, 0.02140 , 0.00986 , 0.01298\n",
            "[INFO] In EPOCH 14 the loss value improved from 0.01942 to 0.01475\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 15/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01341, 0.02200 , 0.00839 , 0.00982\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.01562, 0.02016 , 0.00955 , 0.01715\n",
            "[INFO] In EPOCH 15 the loss value did not improve from 0.01475. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 16/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01339, 0.02217 , 0.00820 , 0.00980\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.01893, 0.02075 , 0.00949 , 0.02654\n",
            "[INFO] In EPOCH 16 the loss value did not improve from 0.01475. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 17/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01285, 0.02150 , 0.00774 , 0.00930\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.02350, 0.02970 , 0.00992 , 0.03089\n",
            "[INFO] In EPOCH 17 the loss value did not improve from 0.01475. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 18/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude): 0.01068, 0.01941 , 0.00629 , 0.00634\n",
            "Val loss (General, Magnitude, Latitude, Longitude): 0.01841, 0.01966 , 0.00869 , 0.02689\n",
            "[INFO] In EPOCH 18 the loss value did not improve from 0.01475. This is the 4 EPOCH in a row.\n",
            "[INFO] Early Stopping the train process. The patience has been exceeded!\n",
            "===========================================================================================\n",
            "[INFO] Total time taken to train the model: 11435.28s\n",
            "[INFO] The best loss value was found in EPOCH 14 where the performance was 0.01475. Model's parameters saved!\n"
          ]
        }
      ],
      "source": [
        "# loop over training epochs\n",
        "\n",
        "early_stopper = EarlyStopping(patience = 4)\n",
        "early_stopper.setCounter(2)\n",
        "early_stopper.setBestLosses([0.05260, 0.05055 , 0.05164 , 0.05562], [0.04710, 0.04473 , 0.04344 , 0.05313])\n",
        "early_stopper.setMinValLoss(0.04710)\n",
        "\n",
        "for e in range(last_epoch, EPOCHS):\n",
        "  # set the model in training model\n",
        "  model.train()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  magnitudeTrainLoss = 0\n",
        "  latitudeTrainLoss = 0\n",
        "  longitudeTrainLoss = 0\n",
        "  generalTrainLoss = 0\n",
        "\n",
        "  magnitudeValLoss = 0\n",
        "  latitudeValLoss = 0\n",
        "  longitudeValLoss = 0\n",
        "  generalValLoss = 0\n",
        "\n",
        "  for sampled_batch in trainDataLoader:\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "    # perform a forward pass and calculate training loss\n",
        "    pred = model(x)\n",
        "    loss = lossFn(pred, y)\n",
        "    # zero out the gradients, perfrom backprop step and update weights\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # add the loss to the total training so far and calculate the\n",
        "    generalTrainLoss += loss.cpu().detach().numpy()\n",
        "    magnitudeTrainLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTrainLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTrainLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  # switch off autograd for evaluation\n",
        "  with torch.no_grad():\n",
        "    # set the model evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # for loop over de validation set\n",
        "    for sampled_batch in valDataLoader:\n",
        "      # send the input to the device\n",
        "      (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "      # make the predictions and calculate valdiation loss\n",
        "      pred = model(x)\n",
        "      generalValLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "      magnitudeValLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "      latitudeValLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "      longitudeValLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "\n",
        "  # Adding another step through an epoch to the scheduler\n",
        "  scheduler.step((generalValLoss / valSteps))\n",
        "\n",
        "  # calculate the average training and validation loss\n",
        "  avgGeneralTrainLoss = generalTrainLoss / trainSteps\n",
        "  avgMagnitudeTrainLoss =  magnitudeTrainLoss / trainSteps\n",
        "  avgLatitudeTrainLoss =  latitudeTrainLoss / trainSteps\n",
        "  avgLongitudeTrainLoss =  longitudeTrainLoss / trainSteps\n",
        "\n",
        "  avgGeneralValLoss = generalValLoss / valSteps\n",
        "  avgMagnitudeValLoss =  magnitudeValLoss / valSteps\n",
        "  avgLatitudeValLoss =  latitudeValLoss / valSteps\n",
        "  avgLongitudeValLoss =  longitudeValLoss / valSteps\n",
        "\n",
        "  # update training and evaluation history\n",
        "  H[\"magnitude_train_loss\"].append(avgMagnitudeTrainLoss)\n",
        "  H[\"latitude_train_loss\"].append(avgLatitudeTrainLoss)\n",
        "  H[\"longitude_train_loss\"].append(avgLongitudeTrainLoss)\n",
        "\n",
        "\n",
        "  H[\"magnitude_val_loss\"].append(avgMagnitudeValLoss)\n",
        "  H[\"latitude_val_loss\"].append(avgLatitudeValLoss)\n",
        "  H[\"longitude_val_loss\"].append(avgLongitudeValLoss)\n",
        "\n",
        "  # saving current state dicts of the epoch and loss history - checkpoint\n",
        "  torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss_history': H\n",
        "            }, f\"{models_dir}/reSnet_train_state_dict.pt\")\n",
        "\n",
        "  # print the model training and validation information\n",
        "  print(\"[INFO] EPOCH: {}/{} ...\".format(e+1, EPOCHS))\n",
        "  print(\"Train loss (General, Magnitude, Latitude, Longitude): {:.5f}, {:.5f} , {:.5f} , {:.5f}\".format(\n",
        "      avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss))\n",
        "  print(\"Val loss (General, Magnitude, Latitude, Longitude): {:.5f}, {:.5f} , {:.5f} , {:.5f}\".format(\n",
        "      avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss))\n",
        "\n",
        "  # checking if resulting loss in evaluation is the best\n",
        "  if early_stopper.earlyStop(avgGeneralValLoss, (e+1), [avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss],\n",
        "                [avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss]):\n",
        "    # if it is not better - stopping train process\n",
        "    print(\"[INFO] Early Stopping the train process. The patience has been exceeded!\")\n",
        "    print(\"===========================================================================================\")\n",
        "    break\n",
        "\n",
        "  print(\"===========================================================================================\")\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime-startTime))\n",
        "print(\"[INFO] The best loss value was found in EPOCH {} where the performance was {:.5f}. Model's parameters saved!\".format(early_stopper.getBestEpoch(), early_stopper.getBestValLosses()[0]))\n",
        "early_stopper.saveLossesLocally()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rprn14X5Ksvh"
      },
      "outputs": [],
      "source": [
        "# plot the training and val losses\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# Plotting magnitude loss on train and evaluation\n",
        "plt.figure(\"magnitude_loss\").clear()\n",
        "plt.plot(H[\"magnitude_train_loss\"], label=\"magnitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"magnitude_val_loss\"], label=\"magnitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Magnitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_magnitude_loss.png\")\n",
        "\n",
        "# Plotting latitude loss on train and evaluation\n",
        "plt.figure(\"latitude_loss\").clear()\n",
        "plt.plot(H[\"latitude_train_loss\"], label=\"latitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"latitude_val_loss\"], label=\"latitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Latitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_latitude_loss.png\")\n",
        "\n",
        "# Plotting longitude loss on train and evaluation\n",
        "plt.figure(\"longitude_loss\").clear()\n",
        "plt.plot(H[\"longitude_train_loss\"], label=\"longitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"longitude_val_loss\"], label=\"longitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Longitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_longitude_loss.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HVoM4I-b9Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1533fcf3-452e-47ac-8431-ca488437c061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "[INFO] Loss/Accuracy values obtained on the test set\n",
            "[INFO] Test loss (General, Magnitude, Latitude, Longitude): 0.01377, 0.01957 , 0.00956 , 0.01218\n",
            "[INFO] R2 Score obtained on the test set: [0.9286793  0.8794159  0.84833235]\n"
          ]
        }
      ],
      "source": [
        "# evaluation of the network in the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "r2_score = R2Score()\n",
        "model.load_state_dict(torch.load(f\"{models_dir}/EqNet_state_dict.pt\"))\n",
        "\n",
        "test_results = {\n",
        "    \"true_values\":[],\n",
        "    \"pred_values\":[]\n",
        "}\n",
        "\n",
        "# turn off autograd for testing evaluation\n",
        "with torch.no_grad():\n",
        "  # set the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  generalTestLoss = 0\n",
        "  magnitudeTestLoss = 0\n",
        "  latitudeTestLoss =  0\n",
        "  longitudeTestLoss = 0\n",
        "\n",
        "  # loop over the test set\n",
        "  for sampled_batch in testDataLoader:\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device),sampled_batch['results'].to(device))\n",
        "    test_results[\"true_values\"].append(y.cpu().detach().numpy().tolist())\n",
        "\n",
        "    # make the predictions and add them to the list\n",
        "    pred = model(x)\n",
        "    test_results[\"pred_values\"].append(pred.cpu().detach().numpy().tolist())\n",
        "\n",
        "    generalTestLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "    magnitudeTestLoss += lossFn(pred[:,0], y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTestLoss += lossFn(pred[:,1], y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTestLoss += lossFn(pred[:,2], y[:,2]).cpu().detach().numpy()\n",
        "    r2score_metric.update(pred, y)\n",
        "\n",
        "  # generate test MSE and R2 Score\n",
        "  avgGeneralTestLoss = generalTestLoss / testSteps\n",
        "  avgMagnitudeTestLoss = magnitudeTestLoss / testSteps\n",
        "  avgLatitudeTestLoss =  latitudeTestLoss / testSteps\n",
        "  avgLongitudeTestLoss = longitudeTestLoss / testSteps\n",
        "\n",
        "  r2score_value = r2score_metric.compute()\n",
        "\n",
        "  print(\"[INFO] Loss/Accuracy values obtained on the test set\")\n",
        "  print(\"[INFO] Test loss (General, Magnitude, Latitude, Longitude): {:.5f}, {:.5f} , {:.5f} , {:.5f}\".format(\n",
        "      avgGeneralTestLoss, avgMagnitudeTestLoss, avgLatitudeTestLoss, avgLongitudeTestLoss))\n",
        "  print(\"[INFO] R2 Score obtained on the test set: {}\".format(r2score_value.cpu().detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeOoMEdVd0nR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2a4a55e-b25b-49a7-d204-4deb262d536f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "Loss obtained on Train, Valdation and Test sets\n",
            "| Metric        |   Train |   Validation |       Test |\n",
            "|---------------|---------|--------------|------------|\n",
            "| General MSE   | 0.01421 |      0.01475 | 0.0137727  |\n",
            "| Magnitude MSE | 0.02265 |      0.0214  | 0.0195746  |\n",
            "| Latitude MSE  | 0.00897 |      0.00986 | 0.00956115 |\n",
            "| Longitude MSE | 0.011   |      0.01298 | 0.0121824  |\n",
            "----------------------------------------------------------------------\n",
            "Accuracy obtained on the Test set\n",
            "| Metric   |   Magnitude |   Latitude |   Longitude |\n",
            "|----------|-------------|------------|-------------|\n",
            "| R2 Score |    0.928679 |   0.879416 |    0.848332 |\n"
          ]
        }
      ],
      "source": [
        "# plotting and saving plots for Y_true - Y_pred\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "\"\"\"\n",
        "puncte = torch.load(f\"{models_dir}/true_pred_points.pt\")\n",
        "\"\"\"\n",
        "test_true = []\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(test_results[\"true_values\"])):\n",
        "  test_true.extend(test_results[\"true_values\"][i])\n",
        "  test_pred.extend(test_results[\"pred_values\"][i])\n",
        "\n",
        "test_true = np.array(test_true)\n",
        "test_pred = np.array(test_pred)\n",
        "\n",
        "\n",
        "torch.save({\n",
        "            'true_points':test_true,\n",
        "            'pred_points':test_pred\n",
        "            }, f\"{models_dir}/true_pred_points.pt\")\n",
        "\n",
        "\n",
        "# Plotting magnitude test Y_true - Y_pred\n",
        "plt.figure(\"magnitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,0], test_pred[:,0], \"or\")\n",
        "plt.title(\"Magnitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_magnitude_true-pred.png\")\n",
        "\n",
        "# Plotting latitude test Y_true - Y_pred\n",
        "plt.figure(\"latitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,1], test_pred[:,1], \"or\")\n",
        "plt.title(\"Latitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_latitude_true-pred.png\")\n",
        "\n",
        "# Plotting longitude test Y_true - Y_pred\n",
        "plt.figure(\"longitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,2], test_pred[:,2], \"or\")\n",
        "plt.title(\"Longitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet_longitude_true-pred.png\")\n",
        "\n",
        "# generating table with best values obtained on train, evaluation and test\n",
        "# early_stopper.loadLossesLocally()\n",
        "\n",
        "models_performance = [] # used to showcase the Loss/Accuracy values obtained\n",
        "models_performance.append([\"General MSE\", round(early_stopper.getBestTrainLosses()[0], 5),  round(early_stopper.getBestValLosses()[0], 5), avgGeneralTestLoss])\n",
        "models_performance.append([\"Magnitude MSE\", round(early_stopper.getBestTrainLosses()[1], 5),  round(early_stopper.getBestValLosses()[1], 5), avgMagnitudeTestLoss])\n",
        "models_performance.append([\"Latitude MSE\", round(early_stopper.getBestTrainLosses()[2], 5),  round(early_stopper.getBestValLosses()[2], 5), avgLatitudeTestLoss])\n",
        "models_performance.append([\"Longitude MSE\", round(early_stopper.getBestTrainLosses()[3], 5),  round(early_stopper.getBestValLosses()[3], 5), avgLongitudeTestLoss])\n",
        "\n",
        "r2_score_list = r2score_value.cpu().detach().numpy().tolist()\n",
        "models_performance.append([\"R2 Score\", r2_score_list[0], r2_score_list[1], r2_score_list[2]])\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Loss obtained on Train, Valdation and Test sets\")\n",
        "print(tabulate(models_performance[0:4], headers=[\"Metric\", \"Train\", \"Validation\", \"Test\"], tablefmt=\"github\"))\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Accuracy obtained on the Test set\")\n",
        "print(tabulate([models_performance[4]], headers=[\"Metric\", \"Magnitude\", \"Latitude\", \"Longitude\"], tablefmt=\"github\"))\n",
        "\n",
        "# serialize the model to disk\n",
        "torch.save(model, f\"{models_dir}/EqNet_model.pt\")\n",
        "\n",
        "# TODO: Plot y_true, y_predicte for test\n",
        "# TODO: Justify hop_length, batch size\n",
        "# TODO: Covariate shift and vanishing gradients\n",
        "# TODO: ResNets, skip connections\n",
        "\n",
        "# Concluzii rezultate obținute.\n",
        "# Cum justific alegerea structurii retelei neuronale?\n",
        "# Ma asteptam sa fac eu ceva de la 0 sau sa modific la o structura existenta,\n",
        "# doar ca nu inteleg cum as putea justifica alegerea facuta?\n",
        "# De invatat mai multe despre ResNets\n",
        "\n",
        "# Cum justific alegerea batch size, epochs si hop_length?\n",
        "# De cautat\n",
        "\n",
        "# Tinand cont ca datasetul este de 41365, ar fi bine sa iau 41360 pentru a fi\n",
        "# exacta impartirea 70% - 15% - 15%? Sau nu are relevanta\n",
        "# Nu\n",
        "\n",
        "# Este ok layer ul final adaugat de mine? (MLP (flattent)-2048-1024-3)\n",
        "# Da\n",
        "\n",
        "# Folosesc tot setul de date sau iau mai putin? Daca iau tot setul de date, ar fi\n",
        "# ok sa iau si un batch size mai mare?\n",
        "# Nu accelereaza antrenarea. Setul de date e perfect asa cum este. Teste pe 10000"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}