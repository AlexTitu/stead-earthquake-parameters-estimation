{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8k8xwh83ZJK",
        "outputId": "cc99a43e-8807-46ae-d325-d4ef455b52ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torcheval) (4.5.0)\n",
            "Collecting torchtnt>=0.0.5\n",
            "  Downloading torchtnt-0.0.7-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyre-extensions\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (67.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (2.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (23.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torchtnt>=0.0.5->torcheval) (2023.4.0)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.2.3)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.53.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.11.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (3.25.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->torchtnt>=0.0.5->torcheval) (6.4.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->torchtnt>=0.0.5->torcheval) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, torchtnt, torcheval\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.0.7 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEzsjsCZ3OaN",
        "outputId": "8d3c39be-2f8e-4c99-ebc7-0d7d9eeea9b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 6000 samples for each waveform recorded for 60 seconds of ground motion - sampling rate 100Hz\n",
        "# 41365 waveforms - size of dataset for the chosen device PB;B082;33.598182;-116.596005;1374.8 HH & EH\n",
        "# Magnitude Type - ml (local) - ~2.0 to ~6.5 magnitudes - 0 - 600 km distance range ;\n",
        "# source latitude + longitude = epicenter + depth = hypocenter\n",
        "\n",
        "# 4 grafice, pentru fiecare loss pentru train si validare (done) + checkpoint\n",
        "# pe test pe train si val - un tabel care sa contina toate marimile + R2 pe test\n",
        "\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "import h5py\n",
        "import numpy as np\n",
        "import librosa as lib\n",
        "import librosa.display as libd\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "plots_dir = '/content/drive/My Drive/Plots/full'\n",
        "models_dir = '/content/drive/My Drive/Models/full'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-70OKCLZ6C2Y"
      },
      "outputs": [],
      "source": [
        "class STEADDataset(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file,hdf5_file,transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        hdf5_file (string): File with all the waveforms.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.tags = pd.read_csv(csv_file)\n",
        "    # self.tags = self.tags [(self.tags.trace_category == 'earthquake_local') & (self.tags.source_distance_km<=20)&(self.tags.source_magnitude > 3)]\n",
        "    self.hdf5_file = hdf5_file\n",
        "    self.traces = self.tags['trace_name'].to_list()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.traces)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dataset = h5py.File(self.hdf5_file,'r')\n",
        "    tracename = self.traces[idx]\n",
        "    waveform = dataset.get('data/'+tracename)\n",
        "    data = np.array(waveform)\n",
        "    spectrograms = self.getSpectrogram(data)\n",
        "\n",
        "    sample = {'spectrograms':spectrograms, 'source_magnitude':waveform.attrs['source_magnitude'],\n",
        "              'source_latitude':waveform.attrs['source_latitude'],'source_longitude':waveform.attrs['source_longitude'],\n",
        "              'source_depth_km':waveform.attrs['source_depth_km']}\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    dataset.close()\n",
        "    return sample\n",
        "\n",
        "  def getSpectrogram(self, waveforms):\n",
        "    # defining axis\n",
        "    EW = waveforms[:,0]\n",
        "    NS = waveforms[:,1]\n",
        "    Vert = waveforms[:,2]\n",
        "\n",
        "    EW_ft = lib.stft(EW,n_fft=1024, hop_length=32) # n_fft = dimensiunea semnalului din fereastra stft = initial 2048\n",
        "    NS_ft = lib.stft(NS,n_fft=1024, hop_length=32)\n",
        "    Vert_ft = lib.stft(Vert,n_fft=1024, hop_length=32) # window = 'hann' # 1024 sau 512\n",
        "    EW_db = lib.amplitude_to_db(np.abs(EW_ft), ref=np.max) # np.max = normare\n",
        "    NS_db = lib.amplitude_to_db(np.abs(NS_ft), ref=np.max)\n",
        "    Vert_db = lib.amplitude_to_db(np.abs(Vert_ft), ref=np.max)\n",
        "\n",
        "    spectrograms = np.array([EW_db, NS_db, Vert_db])\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      if sample['source_depth_km'] == \"None\":\n",
        "        return None\n",
        "      else:\n",
        "        spectrograms = sample['spectrograms']\n",
        "        results = np.array([sample['source_magnitude'], sample['source_latitude'], sample['source_longitude'], sample['source_depth_km']], dtype=np.float32)\n",
        "\n",
        "        return {'spectrograms': torch.from_numpy(spectrograms),\n",
        "                'results':  torch.from_numpy(results)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs8di3641aYp"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "  # Filter out the None samples\n",
        "  filtered_batch = [sample for sample in batch if sample is not None]\n",
        "  if len(filtered_batch) == 0:\n",
        "    # if the batch length is 0 - all are None\n",
        "    return None\n",
        "  else:\n",
        "    # create the new batch with the eliminated None components\n",
        "    return default_collate(filtered_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VIF677lV3bK",
        "outputId": "b99e5f92-868d-44ce-c133-90f471c86da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-7c2a70861e50>:10: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.tags = pd.read_csv(csv_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "41365\n"
          ]
        }
      ],
      "source": [
        "STEAD_dataset = STEADDataset(csv_file=\"drive/My Drive/dataset.csv\",\n",
        "                             hdf5_file=\"drive/My Drive/dataset.hdf5\",\n",
        "                             transform=ToTensor())\n",
        "\n",
        "for i in range(len(STEAD_dataset)):\n",
        "  sample = STEAD_dataset[i]\n",
        "  print(sample['results'].size())\n",
        "  print(sample['spectrograms'].size())\n",
        "  if i == 3:\n",
        "    break\n",
        "\n",
        "print(len(STEAD_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y46scF2YAp65"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module # implement a class rather than using Sequential object\n",
        "from torch.nn import Conv2d # convolutional layer\n",
        "from torch.nn import Linear # Fully connected layers\n",
        "from torch.nn import MaxPool2d # 2D max-pooling to reduce spatial dimensions\n",
        "from torch.nn import ReLU # activation function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Scheduler that reduces learning rate linearly\n",
        "from torch import flatten # Flattens the output of a multidimensional volume (CONV or POOl layer) -> Fully connected layer\n",
        "from torch.utils.data import random_split # for splitting Dataset into Train, Evaluation and Test\n",
        "from torcheval.metrics import R2Score # Evaluates model's accuracy\n",
        "from torch import nn\n",
        "import torch\n",
        "import time # for timing train loop\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFp058dTkagm"
      },
      "outputs": [],
      "source": [
        "# defining model class\n",
        "class EqNet(Module):\n",
        "  def __init__(self, numChannels, outputNodes):\n",
        "    # call the parent constructor\n",
        "    super(EqNet,self).__init__()\n",
        "\n",
        "    # initialize first CONV => RELU => POOL layer\n",
        "    self.conv1 = Conv2d(in_channels=numChannels, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.relu1 = ReLU()\n",
        "    self.maxpool1 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize second CONV => RELU => POOL layer\n",
        "    self.conv2 = Conv2d(in_channels=16, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.relu2 = ReLU()\n",
        "    self.maxpool2 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize first CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    self.conv3 = Conv2d(in_channels=16, out_channels=32, kernel_size=(5,5), padding=(2,2))\n",
        "    self.relu3 = ReLU()\n",
        "    self.conv4 = Conv2d(in_channels=32, out_channels=32, kernel_size=(5,5), padding=(2,2))\n",
        "    self.relu4 = ReLU()\n",
        "    self.maxpool3 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize second CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    self.conv5 = Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3), padding=(1,1))\n",
        "    self.relu5 = ReLU()\n",
        "    self.conv6 = Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3), padding=(1,1))\n",
        "    self.relu6 = ReLU()\n",
        "    self.maxpool4 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize third CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    self.conv7 = Conv2d(in_channels=64, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "    self.relu7 = ReLU()\n",
        "    self.conv8 = Conv2d(in_channels=96, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "    self.relu8 = ReLU()\n",
        "    self.maxpool5 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize last CONV => RELU => CONV => POOL layer\n",
        "    self.conv9 = Conv2d(in_channels=96, out_channels=128, kernel_size=(3,3), padding=(1,1))\n",
        "    self.relu9 = ReLU()\n",
        "    self.conv10 = Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), padding=(1,1))\n",
        "    self.maxpool6 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize first (and only) set of FC => ReLU layers - fully connected layer\n",
        "    self.fc1 = Linear(in_features=2048, out_features=1024)\n",
        "    self.relu10 = ReLU()\n",
        "\n",
        "    # initialize first (and only) set of FC => Linear *Regression* Layers\n",
        "    self.fc2 = Linear(in_features=1024, out_features=outputNodes)\n",
        "\n",
        "  def forward(self, x): # x - batch of input data to the network\n",
        "    # pass the input through the first set of CONV -> ReLU -> POOL layers\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    # pass the input through the second set of CONV -> ReLU -> POOL layer\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    # pass the input through the first CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu4(x)\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    # pass the input through the second CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    x = self.conv5(x)\n",
        "    x = self.relu5(x)\n",
        "    x = self.conv6(x)\n",
        "    x = self.relu6(x)\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    # pass the input through the third CONV => RELU => CONV => RELU => POOL layer = Residual\n",
        "    x = self.conv7(x)\n",
        "    x = self.relu7(x)\n",
        "    x = self.conv8(x)\n",
        "    x = self.relu8(x)\n",
        "    x = self.maxpool5(x)\n",
        "\n",
        "    # pass the input through the last CONV => RELU => CONV => POOL layer\n",
        "    x = self.conv9(x)\n",
        "    x = self.relu9(x)\n",
        "    x = self.conv10(x)\n",
        "    x = self.maxpool6(x)\n",
        "\n",
        "    # flatten the output from the previous layer and pass it through FC layer\n",
        "    x = flatten(x,1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu10(x)\n",
        "\n",
        "    # pass the output to our Linear layer for regression predictions\n",
        "    output = self.fc2(x)\n",
        "\n",
        "    # return the output predictions\n",
        "    return output\n",
        "\n",
        "\n",
        "# defining Early Stopping class\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience = 1, min_delta = 0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation_loss = np.inf\n",
        "    self.best_epoch = 0\n",
        "    self.best_train = [None] * 5\n",
        "    self.best_val = [None] * 5\n",
        "\n",
        "  def earlyStop(self, validation_loss, epoch, TrainLoss, ValLoss):\n",
        "    if validation_loss <= self.min_validation_loss:\n",
        "      print(\"[INFO] In EPOCH {} the loss value improved from {:.5f} to {:.5f}\".format(epoch, self.min_validation_loss, validation_loss))\n",
        "      self.min_validation_loss = validation_loss\n",
        "      self.counter = 0\n",
        "      self.best_epoch = epoch\n",
        "      torch.save(model.state_dict(), f\"{models_dir}/EqNet_state_dict.pt\")\n",
        "      self.setBestLosses(TrainLoss, ValLoss)\n",
        "\n",
        "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "      self.counter += 1\n",
        "      print(\"[INFO] In EPOCH {} the loss value did not improve from {:.5f}. This is the {} EPOCH in a row.\".format(epoch, self.min_validation_loss, self.counter))\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def setCounter(self, counter_state):\n",
        "    self.counter = counter_state\n",
        "\n",
        "  def setMinValLoss(self, ValLoss):\n",
        "    self.min_validation_loss = ValLoss\n",
        "\n",
        "  def setBestLosses(self, TrainLoss, ValLoss):\n",
        "    self.best_train = TrainLoss\n",
        "    self.best_val = ValLoss\n",
        "\n",
        "  def setBestEpoch(self, bestEpoch):\n",
        "    self.best_epoch = bestEpoch\n",
        "\n",
        "  def getBestTrainLosses(self):\n",
        "    return self.best_train\n",
        "\n",
        "  def getBestValLosses(self):\n",
        "    return self.best_val\n",
        "\n",
        "  def getBestEpoch(self):\n",
        "    return self.best_epoch\n",
        "\n",
        "  def saveLossesLocally(self):\n",
        "    np.save(f'{models_dir}/losses_train.npy', np.array(self.best_train))\n",
        "    np.save(f'{models_dir}/losses_val.npy', np.array(self.best_val))\n",
        "\n",
        "  def loadLossesLocally(self):\n",
        "    self.best_train = np.load(f'{models_dir}/losses_train.npy')\n",
        "    self.best_val = np.load(f'{models_dir}/losses_val.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaYTXUM_Ejs",
        "outputId": "5c20f699-5ae0-4b88-f0d1-dc969e7d063f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device used for training...cuda\n"
          ]
        }
      ],
      "source": [
        "# define training hyperparameters\n",
        "INIT_LR = 5.5*1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# define the train and validation splits\n",
        "TRAIN_SPLIT = 0.70\n",
        "VAL_TEST_SPLIT = 0.15\n",
        "\n",
        "# set the device we will be using to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] device used for training...{}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQSr35LAfSk",
        "outputId": "34328888-a875-4d41-f871-25e37d7b252a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] generating the train/validation split...\n"
          ]
        }
      ],
      "source": [
        "# calculate the train/validation split\n",
        "print(\"[INFO] generating the train/validation split...\")\n",
        "numTrainSamples = int(len(STEAD_dataset)*TRAIN_SPLIT)\n",
        "numValSamples = int(len(STEAD_dataset)*VAL_TEST_SPLIT)\n",
        "numTestSamples = int(len(STEAD_dataset)-(numTrainSamples+numValSamples))\n",
        "\n",
        "(trainData, valData, testData) = random_split(STEAD_dataset,[numTrainSamples, numValSamples, numTestSamples], generator=torch.Generator().manual_seed(19))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpR-V_0qNCUW"
      },
      "outputs": [],
      "source": [
        "# initialize the train, validation and test dataloaders\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "\n",
        "# calculate steps per epoch for training, validation set\n",
        "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
        "valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
        "testSteps = len(testDataLoader.dataset) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "espa39G3PHxX",
        "outputId": "ef24ea1e-5fb9-4486-d505-5824f8ab32d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] initializing the EqNet model...\n",
            "[INFO] training the network...\n"
          ]
        }
      ],
      "source": [
        "# load from drive previous results\n",
        "previous_state = torch.load(f\"{models_dir}/train_state_dict.pt\")\n",
        "\n",
        "# initialize the EqNet model\n",
        "print(\"[INFO] initializing the EqNet model...\")\n",
        "model = EqNet(numChannels=3, outputNodes=4).to(device)\n",
        "model.load_state_dict(previous_state['model_state_dict'])\n",
        "\n",
        "# initialize the optimizer and lr_scheduler\n",
        "opt = Adam(model.parameters(), lr=INIT_LR)\n",
        "opt.load_state_dict(previous_state['optimizer_state_dict'])\n",
        "\n",
        "scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=2)\n",
        "scheduler.load_state_dict(previous_state['scheduler_state_dict'])\n",
        "\n",
        "# initialize loss function and accuracy measurement\n",
        "lossFn = nn.MSELoss()\n",
        "r2score_metric = R2Score(multioutput='raw_values', device=device)\n",
        "\n",
        "# initialize a dictionary to store training and evaluation history\n",
        "H = {\n",
        "    \"magnitude_train_loss\":[],\n",
        "    \"latitude_train_loss\":[],\n",
        "    \"longitude_train_loss\":[],\n",
        "    \"depth_train_loss\":[],\n",
        "    \"magnitude_val_loss\":[],\n",
        "    \"latitude_val_loss\":[],\n",
        "    \"longitude_val_loss\":[],\n",
        "    \"depth_val_loss\":[],\n",
        "}\n",
        "\n",
        "# load previous loss history and last epoch\n",
        "H = previous_state['train_loss_history']\n",
        "last_epoch = previous_state['epoch']+1\n",
        "\n",
        "# measure how long training is going to take\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bcz3bGOU-En",
        "outputId": "978d661e-e3f9-4e21-c512-202b1eff2db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] EPOCH: 1/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 37.09297, 0.73689 , 9.76240 , 112.98773, 24.88485\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 11.10263, 0.21707 , 2.32728 , 20.45133, 21.41482\n",
            "[INFO] In EPOCH 1 the loss value improved from inf to 11.10263\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 2/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 6.99838, 0.17706 , 0.73449 , 7.11950, 19.96246\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 4.49787, 0.20874 , 0.10253 , 0.59962, 17.08059\n",
            "[INFO] In EPOCH 2 the loss value improved from 11.10263 to 4.49787\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 3/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 5.69317, 0.18141 , 0.77031 , 7.47463, 14.34633\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 4.83457, 0.13770 , 0.52881 , 6.77442, 11.89733\n",
            "[INFO] In EPOCH 3 the loss value did not improve from 4.49787. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 4/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 4.13429, 0.13265 , 0.59265 , 5.46112, 10.35075\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 2.46649, 0.10650 , 0.13794 , 0.57161, 9.04991\n",
            "[INFO] In EPOCH 4 the loss value improved from 4.49787 to 2.46649\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 5/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 3.32652, 0.10502 , 0.47400 , 4.29653, 8.43052\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 2.51913, 0.06663 , 0.26856 , 1.62155, 8.11981\n",
            "[INFO] In EPOCH 5 the loss value did not improve from 2.46649. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 6/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 3.06528, 0.07990 , 0.47584 , 4.36448, 7.34090\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.86242, 0.05202 , 0.13937 , 0.45954, 6.79874\n",
            "[INFO] In EPOCH 6 the loss value improved from 2.46649 to 1.86242\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 7/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 2.66402, 0.07529 , 0.40214 , 3.62498, 6.55368\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 2.07672, 0.07012 , 0.07176 , 0.33241, 7.83257\n",
            "[INFO] In EPOCH 7 the loss value did not improve from 1.86242. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 8/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 2.25912, 0.07561 , 0.29839 , 2.48961, 6.17285\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.91749, 0.04425 , 0.09333 , 0.61142, 6.92096\n",
            "[INFO] In EPOCH 8 the loss value did not improve from 1.86242. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 9/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 2.03836, 0.08223 , 0.24033 , 1.91094, 5.91994\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 3.55678, 0.05769 , 0.51062 , 5.10086, 8.55796\n",
            "[INFO] In EPOCH 9 the loss value did not improve from 1.86242. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 10/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.17836, 0.04606 , 0.05374 , 0.13365, 4.48001\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.25254, 0.04249 , 0.04834 , 0.08243, 4.83690\n",
            "[INFO] In EPOCH 10 the loss value improved from 1.86242 to 1.25254\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 11/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.09979, 0.04456 , 0.04829 , 0.08871, 4.21757\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.20380, 0.04976 , 0.04717 , 0.08766, 4.63059\n",
            "[INFO] In EPOCH 11 the loss value improved from 1.25254 to 1.20380\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 12/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.08345, 0.04516 , 0.04813 , 0.09498, 4.14551\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.20858, 0.05102 , 0.04564 , 0.07203, 4.66563\n",
            "[INFO] In EPOCH 12 the loss value did not improve from 1.20380. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 13/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.05118, 0.04440 , 0.04741 , 0.09998, 4.01294\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.24187, 0.03953 , 0.05632 , 0.11870, 4.75292\n",
            "[INFO] In EPOCH 13 the loss value did not improve from 1.20380. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 14/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.03522, 0.04328 , 0.04966 , 0.13188, 3.91607\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.18356, 0.05164 , 0.04457 , 0.10175, 4.53630\n",
            "[INFO] In EPOCH 14 the loss value improved from 1.20380 to 1.18356\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 15/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 1.01528, 0.04266 , 0.04802 , 0.13213, 3.83832\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.25154, 0.03935 , 0.04462 , 0.18547, 4.73671\n",
            "[INFO] In EPOCH 15 the loss value did not improve from 1.18356. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 16/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.98588, 0.04211 , 0.04587 , 0.11319, 3.74234\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.23368, 0.04190 , 0.06205 , 0.30486, 4.52592\n",
            "[INFO] In EPOCH 16 the loss value did not improve from 1.18356. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 17/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.96283, 0.04077 , 0.04744 , 0.14215, 3.62095\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.14122, 0.03700 , 0.03990 , 0.06258, 4.42539\n",
            "[INFO] In EPOCH 17 the loss value improved from 1.18356 to 1.14122\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 18/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.92524, 0.03980 , 0.04379 , 0.10944, 3.50792\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.20738, 0.03850 , 0.05106 , 0.33036, 4.40961\n",
            "[INFO] In EPOCH 18 the loss value did not improve from 1.14122. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 19/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.89767, 0.03902 , 0.04312 , 0.11539, 3.39316\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.19277, 0.03639 , 0.04167 , 0.09976, 4.59325\n",
            "[INFO] In EPOCH 19 the loss value did not improve from 1.14122. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 20/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.86874, 0.03862 , 0.04364 , 0.12574, 3.26695\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.25376, 0.03643 , 0.03872 , 0.13037, 4.80951\n",
            "[INFO] In EPOCH 20 the loss value did not improve from 1.14122. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 21/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.75351, 0.03453 , 0.03436 , 0.04989, 2.89525\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11670, 0.03478 , 0.03616 , 0.04865, 4.34723\n",
            "[INFO] In EPOCH 21 the loss value improved from 1.14122 to 1.11670\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 22/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.73951, 0.03421 , 0.03427 , 0.04941, 2.84013\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11510, 0.03387 , 0.03612 , 0.05115, 4.33926\n",
            "[INFO] In EPOCH 22 the loss value improved from 1.11670 to 1.11510\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 23/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.73260, 0.03393 , 0.03397 , 0.04811, 2.81438\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11448, 0.03692 , 0.03696 , 0.05706, 4.32701\n",
            "[INFO] In EPOCH 23 the loss value improved from 1.11510 to 1.11448\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 24/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.72763, 0.03378 , 0.03386 , 0.04955, 2.79334\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.14022, 0.03534 , 0.03599 , 0.04826, 4.44131\n",
            "[INFO] In EPOCH 24 the loss value did not improve from 1.11448. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 25/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.72244, 0.03351 , 0.03409 , 0.05029, 2.77186\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.12222, 0.03375 , 0.03537 , 0.04803, 4.37172\n",
            "[INFO] In EPOCH 25 the loss value did not improve from 1.11448. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 26/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.71773, 0.03343 , 0.03382 , 0.05002, 2.75365\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.12210, 0.03264 , 0.03810 , 0.06712, 4.35053\n",
            "[INFO] In EPOCH 26 the loss value did not improve from 1.11448. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 27/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.70137, 0.03283 , 0.03281 , 0.04325, 2.69660\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11172, 0.03244 , 0.03493 , 0.04592, 4.33358\n",
            "[INFO] In EPOCH 27 the loss value improved from 1.11448 to 1.11172\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 28/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.69973, 0.03286 , 0.03287 , 0.04303, 2.69016\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11340, 0.03249 , 0.03514 , 0.04635, 4.33961\n",
            "[INFO] In EPOCH 28 the loss value did not improve from 1.11172. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 29/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.69895, 0.03281 , 0.03281 , 0.04345, 2.68673\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11286, 0.03243 , 0.03492 , 0.04754, 4.33657\n",
            "[INFO] In EPOCH 29 the loss value did not improve from 1.11172. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 30/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.69811, 0.03279 , 0.03282 , 0.04326, 2.68358\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11312, 0.03248 , 0.03505 , 0.04676, 4.33817\n",
            "[INFO] In EPOCH 30 the loss value did not improve from 1.11172. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 31/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.69636, 0.03275 , 0.03270 , 0.04231, 2.67767\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.11251, 0.03247 , 0.03489 , 0.04574, 4.33694\n",
            "[INFO] In EPOCH 31 the loss value did not improve from 1.11172. This is the 4 EPOCH in a row.\n",
            "[INFO] Early Stopping the train process. The patience has been exceeded!\n",
            "===========================================================================================\n",
            "[INFO] Total time taken to train the model: 17178.83s\n",
            "[INFO] The best loss value was found in EPOCH 27 where the performance was 1.11172. Model's parameters saved!\n"
          ]
        }
      ],
      "source": [
        "# loop over training epochs\n",
        "\n",
        "early_stopper = EarlyStopping(patience = 4)\n",
        "early_stopper.setBestLosses([0.70137, 0.03283 , 0.03281 , 0.04325, 2.69660],[1.11172, 0.03244 , 0.03493 , 0.04592, 4.33358])\n",
        "early_stopper.setBestEpoch(27)\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  # set the model in training model\n",
        "  model.train()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  magnitudeTrainLoss = 0\n",
        "  latitudeTrainLoss = 0\n",
        "  longitudeTrainLoss = 0\n",
        "  depthTrainLoss = 0\n",
        "  generalTrainLoss = 0\n",
        "\n",
        "  magnitudeValLoss = 0\n",
        "  latitudeValLoss = 0\n",
        "  longitudeValLoss = 0\n",
        "  depthValLoss = 0\n",
        "  generalValLoss = 0\n",
        "\n",
        "  for sampled_batch in trainDataLoader:\n",
        "    if sampled_batch is None:\n",
        "      continue\n",
        "\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "    # perform a forward pass and calculate training loss\n",
        "    pred = model(x)\n",
        "    loss = lossFn(pred, y)\n",
        "    # zero out the gradients, perfrom backprop step and update weights\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # add the loss to the total training so far\n",
        "    generalTrainLoss += loss.cpu().detach().numpy()\n",
        "    magnitudeTrainLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTrainLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTrainLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "    depthTrainLoss += lossFn(pred[:,3],y[:,3]).cpu().detach().numpy()\n",
        "\n",
        "  # switch off autograd for evaluation\n",
        "  with torch.no_grad():\n",
        "    # set the model evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # for loop over de validation set\n",
        "    for sampled_batch in valDataLoader:\n",
        "      if sampled_batch is None:\n",
        "        continue\n",
        "\n",
        "      # send the input to the device\n",
        "      (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "      # make the predictions and calculate valdiation loss\n",
        "      pred = model(x)\n",
        "      generalValLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "      magnitudeValLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "      latitudeValLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "      longitudeValLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "      depthValLoss += lossFn(pred[:,3],y[:,3]).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  # calculate the average training and validation loss\n",
        "  avgGeneralTrainLoss = generalTrainLoss / trainSteps\n",
        "  avgMagnitudeTrainLoss =  magnitudeTrainLoss / trainSteps\n",
        "  avgLatitudeTrainLoss =  latitudeTrainLoss / trainSteps\n",
        "  avgLongitudeTrainLoss =  longitudeTrainLoss / trainSteps\n",
        "  avgDepthTrainLoss =  depthTrainLoss / trainSteps\n",
        "\n",
        "  avgGeneralValLoss = generalValLoss / valSteps\n",
        "  avgMagnitudeValLoss =  magnitudeValLoss / valSteps\n",
        "  avgLatitudeValLoss =  latitudeValLoss / valSteps\n",
        "  avgLongitudeValLoss =  longitudeValLoss / valSteps\n",
        "  avgDepthValLoss =  depthValLoss / valSteps\n",
        "\n",
        "  # Adding another step through an epoch to the scheduler\n",
        "  scheduler.step(avgGeneralValLoss)\n",
        "\n",
        "  # update training and evaluation history\n",
        "  H[\"magnitude_train_loss\"].append(avgMagnitudeTrainLoss)\n",
        "  H[\"latitude_train_loss\"].append(avgLatitudeTrainLoss)\n",
        "  H[\"longitude_train_loss\"].append(avgLongitudeTrainLoss)\n",
        "  H[\"depth_train_loss\"].append(avgDepthTrainLoss)\n",
        "\n",
        "\n",
        "  H[\"magnitude_val_loss\"].append(avgMagnitudeValLoss)\n",
        "  H[\"latitude_val_loss\"].append(avgLatitudeValLoss)\n",
        "  H[\"longitude_val_loss\"].append(avgLongitudeValLoss)\n",
        "  H[\"depth_val_loss\"].append(avgDepthValLoss)\n",
        "\n",
        "  # saving current state dicts of the epoch and loss history - checkpoint\n",
        "  torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss_history': H\n",
        "            }, f\"{models_dir}/train_state_dict.pt\")\n",
        "\n",
        "  # print the model training and validation information\n",
        "  print(\"[INFO] EPOCH: {}/{} ...\".format(e+1, EPOCHS))\n",
        "  print(\"Train loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss, avgDepthTrainLoss))\n",
        "  print(\"Val loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss, avgDepthValLoss))\n",
        "\n",
        "  # checking if resulting loss in evaluation is the best\n",
        "  if early_stopper.earlyStop(avgGeneralValLoss, (e+1), [avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss, avgDepthTrainLoss],\n",
        "                [avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss, avgDepthValLoss]):\n",
        "    # if it is not better - stopping train process\n",
        "    print(\"[INFO] Early Stopping the train process. The patience has been exceeded!\")\n",
        "    print(\"===========================================================================================\")\n",
        "    break\n",
        "\n",
        "  print(\"===========================================================================================\")\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime-startTime))\n",
        "print(\"[INFO] The best loss value was found in EPOCH {} where the performance was {:.5f}. Model's parameters saved!\".format(early_stopper.getBestEpoch(), early_stopper.getBestValLosses()[0]))\n",
        "early_stopper.saveLossesLocally()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rprn14X5Ksvh"
      },
      "outputs": [],
      "source": [
        "# plot the training and val losses\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# Plotting magnitude loss on train and evaluation\n",
        "plt.figure(\"magnitude_loss\").clear()\n",
        "plt.plot(H[\"magnitude_train_loss\"], label=\"magnitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"magnitude_val_loss\"], label=\"magnitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Magnitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_magnitude_loss.png\")\n",
        "\n",
        "# Plotting latitude loss on train and evaluation\n",
        "plt.figure(\"latitude_loss\").clear()\n",
        "plt.plot(H[\"latitude_train_loss\"][1:], label=\"latitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"latitude_val_loss\"][1:], label=\"latitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Latitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_latitude_loss2.png\")\n",
        "\n",
        "# Plotting longitude loss on train and evaluation\n",
        "plt.figure(\"longitude_loss\").clear()\n",
        "plt.plot(H[\"longitude_train_loss\"][1:], label=\"longitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"longitude_val_loss\"][1:], label=\"longitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Longitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_longitude_loss2.png\")\n",
        "\n",
        "# Plotting depth loss on train and evaluation\n",
        "plt.figure(\"depth_loss\").clear()\n",
        "plt.plot(H[\"depth_train_loss\"], label=\"depth_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"depth_val_loss\"], label=\"depth_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Depth Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_depth_loss.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HVoM4I-b9Wc",
        "outputId": "ccadd2a2-c574-40d5-b923-d0c25b76cef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "[INFO] Loss/Accuracy values obtained on the test set\n",
            "[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): 1.07620, 0.03122 , 0.03439 , 0.04262, 4.19656\n",
            "[INFO] R2 Score obtained on the test set: [0.8862462  0.5661469  0.46937877 0.7915708 ]\n"
          ]
        }
      ],
      "source": [
        "# evaluation of the network in the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "r2_score = R2Score()\n",
        "model.load_state_dict(torch.load(f\"{models_dir}/EqNet_state_dict.pt\"))\n",
        "\n",
        "test_results = {\n",
        "    \"true_values\":[],\n",
        "    \"pred_values\":[]\n",
        "}\n",
        "\n",
        "# turn off autograd for testing evaluation\n",
        "with torch.no_grad():\n",
        "  # set the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  generalTestLoss = 0\n",
        "  magnitudeTestLoss = 0\n",
        "  latitudeTestLoss =  0\n",
        "  longitudeTestLoss = 0\n",
        "  depthTestLoss = 0\n",
        "\n",
        "  # loop over the test set\n",
        "  for sampled_batch in testDataLoader:\n",
        "    if sampled_batch is None:\n",
        "      continue\n",
        "\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device),sampled_batch['results'].to(device))\n",
        "    test_results[\"true_values\"].append(y.cpu().detach().numpy().tolist())\n",
        "\n",
        "    # make the predictions and add them to the list\n",
        "    pred = model(x)\n",
        "    test_results[\"pred_values\"].append(pred.cpu().detach().numpy().tolist())\n",
        "\n",
        "    generalTestLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "    magnitudeTestLoss += lossFn(pred[:,0], y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTestLoss += lossFn(pred[:,1], y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTestLoss += lossFn(pred[:,2], y[:,2]).cpu().detach().numpy()\n",
        "    depthTestLoss += lossFn(pred[:,3], y[:,3]).cpu().detach().numpy()\n",
        "    r2score_metric.update(pred, y)\n",
        "\n",
        "  # generate test MSE and R2 Score\n",
        "  avgGeneralTestLoss = generalTestLoss / testSteps\n",
        "  avgMagnitudeTestLoss = magnitudeTestLoss / testSteps\n",
        "  avgLatitudeTestLoss =  latitudeTestLoss / testSteps\n",
        "  avgLongitudeTestLoss = longitudeTestLoss / testSteps\n",
        "  avgDepthTestLoss =  depthTestLoss / testSteps\n",
        "\n",
        "  r2score_value = r2score_metric.compute()\n",
        "\n",
        "  print(\"[INFO] Loss/Accuracy values obtained on the test set\")\n",
        "  print(\"[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralTestLoss, avgMagnitudeTestLoss, avgLatitudeTestLoss, avgLongitudeTestLoss, avgDepthTestLoss))\n",
        "  print(\"[INFO] R2 Score obtained on the test set: {}\".format(r2score_value.cpu().detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LeOoMEdVd0nR",
        "outputId": "0db069c5-ae99-4539-9726-56a1fe0dafef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3b7bd6cb7cbe>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mmodels_performance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# used to showcase the Loss/Accuracy values obtained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mmodels_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"General MSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestTrainLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestValLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgGeneralTestLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mmodels_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Magnitude MSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestTrainLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestValLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgMagnitudeTestLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mmodels_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Latitude MSE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestTrainLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBestValLosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavgLatitudeTestLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'avgGeneralTestLoss' is not defined"
          ]
        }
      ],
      "source": [
        "# plotting and saving plots for Y_true - Y_pred\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "\"\"\"\n",
        "puncte = torch.load(f\"{models_dir}/true_pred_points.pt\")\n",
        "\n",
        "test_true = []\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(test_results[\"true_values\"])):\n",
        "  test_true.extend(test_results[\"true_values\"][i])\n",
        "  test_pred.extend(test_results[\"pred_values\"][i])\n",
        "\n",
        "test_true = np.array(test_true)\n",
        "test_pred = np.array(test_pred)\n",
        "\n",
        "\n",
        "torch.save({\n",
        "            'true_points':test_true,\n",
        "            'pred_points':test_pred\n",
        "            }, f\"{models_dir}/true_pred_points.pt\")\n",
        "\n",
        "\n",
        "# Plotting magnitude test Y_true - Y_pred\n",
        "plt.figure(\"magnitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,0], test_pred[:,0], \"or\")\n",
        "plt.title(\"Magnitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_magnitude_true-pred.png\")\n",
        "\n",
        "# Plotting latitude test Y_true - Y_pred\n",
        "plt.figure(\"latitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,1], test_pred[:,1], \"or\")\n",
        "plt.title(\"Latitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_latitude_true-pred.png\")\n",
        "\n",
        "# Plotting longitude test Y_true - Y_pred\n",
        "plt.figure(\"longitude_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,2], test_pred[:,2], \"or\")\n",
        "plt.title(\"Longitude Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_longitude_true-pred.png\")\n",
        "\n",
        "# Plotting depth test Y_true - Y_pred\n",
        "plt.figure(\"depth_true-pred\").clear()\n",
        "plt.plot()\n",
        "plt.plot(test_true[:,3], test_pred[:,3], \"or\")\n",
        "plt.title(\"Depth Y_pred vs Y_true\")\n",
        "plt.xlabel(\"Y_true\")\n",
        "plt.ylabel(\"Y_predicted\")\n",
        "plt.savefig(f\"{plots_dir}/EqNet_depth_true-pred.png\")\n",
        "\"\"\"\n",
        "early_stopper = EarlyStopping(patience = 4)\n",
        "early_stopper.setBestLosses([0.70137, 0.03283 , 0.03281 , 0.04325, 2.69660],[1.11172, 0.03244 , 0.03493 , 0.04592, 4.33358])\n",
        "early_stopper.setBestEpoch(27)\n",
        "\n",
        "# generating table with best values obtained on train, evaluation and test\n",
        "early_stopper.loadLossesLocally()\n",
        "\n",
        "models_performance = [] # used to showcase the Loss/Accuracy values obtained\n",
        "models_performance.append([\"General MSE\", round(early_stopper.getBestTrainLosses()[0], 5),  round(early_stopper.getBestValLosses()[0], 5), avgGeneralTestLoss])\n",
        "models_performance.append([\"Magnitude MSE\", round(early_stopper.getBestTrainLosses()[1], 5),  round(early_stopper.getBestValLosses()[1], 5), avgMagnitudeTestLoss])\n",
        "models_performance.append([\"Latitude MSE\", round(early_stopper.getBestTrainLosses()[2], 5),  round(early_stopper.getBestValLosses()[2], 5), avgLatitudeTestLoss])\n",
        "models_performance.append([\"Longitude MSE\", round(early_stopper.getBestTrainLosses()[3], 5),  round(early_stopper.getBestValLosses()[3], 5), avgLongitudeTestLoss])\n",
        "models_performance.append([\"Depth MSE\", round(early_stopper.getBestTrainLosses()[4], 5),  round(early_stopper.getBestValLosses()[4], 5), avgDepthTestLoss])\n",
        "\n",
        "r2_score_list = r2score_value.cpu().detach().numpy().tolist()\n",
        "models_performance.append([\"R2 Score\", r2_score_list[0], r2_score_list[1], r2_score_list[2], r2_score_list[3]])\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Loss obtained on Train, Valdation and Test sets\")\n",
        "print(tabulate(models_performance[0:5], headers=[\"Metric\", \"Train\", \"Validation\", \"Test\"], tablefmt=\"github\"))\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Accuracy obtained on the Test set\")\n",
        "print(tabulate([models_performance[5]], headers=[\"Metric\", \"Magnitude\", \"Latitude\", \"Longitude\", \"Depth\"], tablefmt=\"github\"))\n",
        "\n",
        "# serialize the model to disk\n",
        "torch.save(model, f\"{models_dir}/EqNet_model.pt\")\n",
        "\n",
        "# TODO: Justify hop_length, batch size\n",
        "# TODO: Covariate shift and vanishing gradients\n",
        "# TODO: ResNets, skip connections\n",
        "\n",
        "# Concluzii rezultate obținute.\n",
        "# Cum justific alegerea structurii retelei neuronale?\n",
        "# Ma asteptam sa fac eu ceva de la 0 sau sa modific la o structura existenta,\n",
        "# doar ca nu inteleg cum as putea justifica alegerea facuta?\n",
        "# De invatat mai multe despre ResNets\n",
        "\n",
        "# Cum justific alegerea batch size, epochs si hop_length?\n",
        "# De cautat\n",
        "\n",
        "# Tinand cont ca datasetul este de 41365, ar fi bine sa iau 41360 pentru a fi\n",
        "# exacta impartirea 70% - 15% - 15%? Sau nu are relevanta\n",
        "# Nu\n",
        "\n",
        "# Este ok layer ul final adaugat de mine? (MLP (flattent)-2048-1024-3)\n",
        "# Da\n",
        "\n",
        "# Folosesc tot setul de date sau iau mai putin? Daca iau tot setul de date, ar fi\n",
        "# ok sa iau si un batch size mai mare?\n",
        "# Nu accelereaza antrenarea. Setul de date e perfect asa cum este. Teste pe 10000"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}