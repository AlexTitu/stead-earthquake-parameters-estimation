{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8k8xwh83ZJK",
        "outputId": "8d7c2681-72bf-4aa9-e5ef-9f5c55455e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtnt>=0.0.5 (from torcheval)\n",
            "  Downloading torchtnt-0.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (5.9.5)\n",
            "Collecting pyre-extensions (from torchtnt>=0.0.5->torcheval)\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
            "Collecting typing-inspect (from pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (16.0.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, torchtnt, torcheval\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.1.0 typing-inspect-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEzsjsCZ3OaN",
        "outputId": "6551bc34-c31d-44c9-ae05-499292a743a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 6000 samples for each waveform recorded for 60 seconds of ground motion - sampling rate 100Hz\n",
        "# 41365 waveforms - size of dataset for the chosen device PB;B082;33.598182;-116.596005;1374.8 HH & EH\n",
        "# Magnitude Type - ml (local) - ~2.0 to ~6.5 magnitudes - 0 - 600 km distance range ;\n",
        "# source latitude + longitude = epicenter + depth = hypocenter\n",
        "\n",
        "# 4 grafice, pentru fiecare loss pentru train si validare (done) + checkpoint\n",
        "# pe test pe train si val - un tabel care sa contina toate marimile + R2 pe test\n",
        "\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "import h5py\n",
        "import numpy as np\n",
        "import librosa as lib\n",
        "import librosa.display as libd\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "plots_dir = '/content/drive/My Drive/Plots/full'\n",
        "models_dir = '/content/drive/My Drive/Models/full'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-70OKCLZ6C2Y"
      },
      "outputs": [],
      "source": [
        "class STEADDataset(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file,hdf5_file,transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): Path to the csv file with annotations.\n",
        "        hdf5_file (string): File with all the waveforms.\n",
        "        transform (callable, optional): Optional transform to be applied on a sample.\n",
        "    \"\"\"\n",
        "    self.tags = pd.read_csv(csv_file)\n",
        "    # self.tags = self.tags [(self.tags.trace_category == 'earthquake_local') & (self.tags.source_distance_km<=20)&(self.tags.source_magnitude > 3)]\n",
        "    self.hdf5_file = hdf5_file\n",
        "    self.traces = self.tags['trace_name'].to_list()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.traces)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dataset = h5py.File(self.hdf5_file,'r')\n",
        "    tracename = self.traces[idx]\n",
        "    waveform = dataset.get('data/'+tracename)\n",
        "    data = np.array(waveform)\n",
        "    spectrograms = self.getSpectrogram(data)\n",
        "\n",
        "    sample = {'spectrograms':spectrograms, 'source_magnitude':waveform.attrs['source_magnitude'],\n",
        "              'source_latitude':waveform.attrs['source_latitude'],'source_longitude':waveform.attrs['source_longitude'],\n",
        "              'source_depth_km':waveform.attrs['source_depth_km']}\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    dataset.close()\n",
        "    return sample\n",
        "\n",
        "  def getSpectrogram(self, waveforms):\n",
        "    # defining axis\n",
        "    EW = waveforms[:,0]\n",
        "    NS = waveforms[:,1]\n",
        "    Vert = waveforms[:,2]\n",
        "\n",
        "    EW_ft = lib.stft(EW,n_fft=1024, hop_length=32) # n_fft = dimensiunea semnalului din fereastra stft = initial 2048\n",
        "    NS_ft = lib.stft(NS,n_fft=1024, hop_length=32)\n",
        "    Vert_ft = lib.stft(Vert,n_fft=1024, hop_length=32) # window = 'hann' # 1024 sau 512\n",
        "    EW_db = lib.amplitude_to_db(np.abs(EW_ft), ref=np.max) # np.max = normare\n",
        "    NS_db = lib.amplitude_to_db(np.abs(NS_ft), ref=np.max)\n",
        "    Vert_db = lib.amplitude_to_db(np.abs(Vert_ft), ref=np.max)\n",
        "\n",
        "    spectrograms = np.array([EW_db, NS_db, Vert_db])\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      if sample['source_depth_km'] == \"None\":\n",
        "        return None\n",
        "      else:\n",
        "        spectrograms = sample['spectrograms']\n",
        "        results = np.array([sample['source_magnitude'], sample['source_latitude'], sample['source_longitude'], sample['source_depth_km']], dtype=np.float32)\n",
        "\n",
        "        return {'spectrograms': torch.from_numpy(spectrograms),\n",
        "                'results':  torch.from_numpy(results)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs8di3641aYp"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "  # Filter out the None samples\n",
        "  filtered_batch = [sample for sample in batch if sample is not None]\n",
        "  if len(filtered_batch) == 0:\n",
        "    # if the batch length is 0 - all are None\n",
        "    return None\n",
        "  else:\n",
        "    # create the new batch with the eliminated None components\n",
        "    return default_collate(filtered_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VIF677lV3bK",
        "outputId": "1e0b6aff-031c-4051-b0ca-96e83d2ee7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fb486d307ea4>:10: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.tags = pd.read_csv(csv_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "torch.Size([4])\n",
            "torch.Size([3, 513, 188])\n",
            "41365\n"
          ]
        }
      ],
      "source": [
        "STEAD_dataset = STEADDataset(csv_file=\"drive/My Drive/dataset.csv\",\n",
        "                             hdf5_file=\"drive/My Drive/dataset.hdf5\",\n",
        "                             transform=ToTensor())\n",
        "\n",
        "for i in range(len(STEAD_dataset)):\n",
        "  sample = STEAD_dataset[i]\n",
        "  print(sample['results'].size())\n",
        "  print(sample['spectrograms'].size())\n",
        "  if i == 3:\n",
        "    break\n",
        "\n",
        "print(len(STEAD_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y46scF2YAp65"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module # implement a class rather than using Sequential object\n",
        "from torch.nn import Conv2d # convolutional layer\n",
        "from torch.nn import BatchNorm2d #BatchNormalization\n",
        "from torch.nn import Linear # Fully connected layers\n",
        "from torch.nn import MaxPool2d # 2D max-pooling to reduce spatial dimensions\n",
        "from torch.nn import ReLU # activation function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # Scheduler that reduces learning rate on plateau\n",
        "from torch import flatten # Flattens the output of a multidimensional volume (CONV or POOl layer) -> Fully connected layer\n",
        "from torch.utils.data import random_split # for splitting Dataset into Train, Evaluation and Test\n",
        "from torcheval.metrics import R2Score # Evaluates model's accuracy\n",
        "from torch import nn\n",
        "import torch\n",
        "import time # for timing train loop\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFp058dTkagm"
      },
      "outputs": [],
      "source": [
        "# defining residual block class\n",
        "class block(Module):\n",
        "  # identity_downsample = conv layers which we might need to do depending on if we ve changed input size or channel numbers\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(5,5), padding=(2,2)):\n",
        "    super(block, self).__init__()\n",
        "    self.conv1 = Conv2d(in_channels, out_channels, kernel_size= kernel_size, padding=padding)\n",
        "    self.bn1 = BatchNorm2d(out_channels)\n",
        "    self.conv2 = Conv2d(out_channels, out_channels, kernel_size= kernel_size, padding=padding)\n",
        "    self.bn2 = BatchNorm2d(out_channels)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    identity = x\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x += identity\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    output = self.maxpool(x)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "class ResNet(Module):\n",
        "  def __init__(self, block, numChannels, outputNodes):\n",
        "    super(ResNet, self).__init__()\n",
        "    # initialize first CONV => RELU => POOL layer\n",
        "    self.conv1 = Conv2d(in_channels=numChannels, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.bn1 = BatchNorm2d(16)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initialize second CONV => RELU => POOL layer\n",
        "    self.conv2 = Conv2d(in_channels=16, out_channels=16, kernel_size=(7,7), padding=(3,3))\n",
        "    self.bn2 = BatchNorm2d(16)\n",
        "\n",
        "    # ResNet layers\n",
        "    self.layer1 = block(in_channels=16, out_channels=32, kernel_size=(5,5), padding=(2,2))\n",
        "    self.layer2 = block(in_channels=32, out_channels=64, kernel_size=(3,3), padding=(1,1))\n",
        "    self.layer3 = block(in_channels=64, out_channels=96, kernel_size=(3,3), padding=(1,1))\n",
        "\n",
        "    # initialize last CONV => RELU => CONV => POOL layer\n",
        "    self.conv3 = Conv2d(in_channels=96, out_channels=128, kernel_size=(3,3), padding=(1,1))\n",
        "    self.bn3 = BatchNorm2d(128)\n",
        "    self.conv4 = Conv2d(in_channels=128, out_channels=128, kernel_size=(3,3), padding=(1,1))\n",
        "    self.bn4 = BatchNorm2d(128)\n",
        "\n",
        "    # initialize first (and only) set of FC => ReLU layers - fully connected layer\n",
        "    self.fc1 = Linear(in_features=2048, out_features=1024)\n",
        "\n",
        "    # initialize first (and only) set of FC => Linear *Regression* Layers\n",
        "    self.fc2 = Linear(in_features=1024, out_features=outputNodes)\n",
        "\n",
        "  def forward(self, x):\n",
        "  # pass the input through the first set of CONV -> ReLU -> POOL layers\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # pass the input through the second set of CONV -> ReLU -> POOL layer\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # pass the input through resnet\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "\n",
        "    # pass the input through the last CONV => RELU => CONV => POOL layer\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # flatten the output from the previous layer and pass it through FC layer\n",
        "    x = flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # pass the output to our Linear layer for regression predictions\n",
        "    output = self.fc2(x)\n",
        "\n",
        "    # return the output predictions\n",
        "    return output\n",
        "\n",
        "\n",
        "# defining Early Stopping class\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience = 1, min_delta = 0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation_loss = np.inf\n",
        "    self.best_epoch = 0\n",
        "    self.best_train = [None] * 5\n",
        "    self.best_val = [None] * 5\n",
        "\n",
        "  def earlyStop(self, validation_loss, epoch, TrainLoss, ValLoss):\n",
        "    if validation_loss <= self.min_validation_loss:\n",
        "      print(\"[INFO] In EPOCH {} the loss value improved from {:.5f} to {:.5f}\".format(epoch, self.min_validation_loss, validation_loss))\n",
        "      self.min_validation_loss = validation_loss\n",
        "      self.counter = 0\n",
        "      self.best_epoch = epoch\n",
        "      torch.save(model.state_dict(), f\"{models_dir}/ResNet3_state_dict.pt\")\n",
        "      self.setBestLosses(TrainLoss, ValLoss)\n",
        "\n",
        "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "      self.counter += 1\n",
        "      print(\"[INFO] In EPOCH {} the loss value did not improve from {:.5f}. This is the {} EPOCH in a row.\".format(epoch, self.min_validation_loss, self.counter))\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def setCounter(self, counter_state):\n",
        "    self.counter = counter_state\n",
        "\n",
        "  def setMinValLoss(self, ValLoss):\n",
        "    self.min_validation_loss = ValLoss\n",
        "\n",
        "  def setBestLosses(self, TrainLoss, ValLoss):\n",
        "    self.best_train = TrainLoss\n",
        "    self.best_val = ValLoss\n",
        "\n",
        "  def setBestEpoch(self, bestEpoch):\n",
        "    self.best_epoch = bestEpoch\n",
        "\n",
        "  def getBestTrainLosses(self):\n",
        "    return self.best_train\n",
        "\n",
        "  def getBestValLosses(self):\n",
        "    return self.best_val\n",
        "\n",
        "  def getBestEpoch(self):\n",
        "    return self.best_epoch\n",
        "\n",
        "  def saveLossesLocally(self):\n",
        "    np.save(f'{models_dir}/losses_train_3.npy', np.array(self.best_train))\n",
        "    np.save(f'{models_dir}/losses_val_3.npy', np.array(self.best_val))\n",
        "\n",
        "  def loadLossesLocally(self):\n",
        "    self.best_train = np.load(f'{models_dir}/losses_train_3.npy')\n",
        "    self.best_val = np.load(f'{models_dir}/losses_val_3.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaYTXUM_Ejs",
        "outputId": "2264849e-762e-4d49-c929-d964f213929b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device used for training...cuda\n"
          ]
        }
      ],
      "source": [
        "# define training hyperparameters\n",
        "INIT_LR = 5*1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "\n",
        "# define the train and validation splits\n",
        "TRAIN_SPLIT = 0.70\n",
        "VAL_TEST_SPLIT = 0.15\n",
        "\n",
        "# set the device we will be using to train the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] device used for training...{}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQSr35LAfSk",
        "outputId": "0615dffe-0cbe-463c-a12a-f679f5f5fbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] generating the train/validation split...\n"
          ]
        }
      ],
      "source": [
        "# calculate the train/validation split\n",
        "print(\"[INFO] generating the train/validation split...\")\n",
        "numTrainSamples = int(len(STEAD_dataset)*TRAIN_SPLIT)\n",
        "numValSamples = int(len(STEAD_dataset)*VAL_TEST_SPLIT)\n",
        "numTestSamples = int(len(STEAD_dataset)-(numTrainSamples+numValSamples))\n",
        "\n",
        "(trainData, valData, testData) = random_split(STEAD_dataset,[numTrainSamples, numValSamples, numTestSamples], generator=torch.Generator().manual_seed(19))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpR-V_0qNCUW"
      },
      "outputs": [],
      "source": [
        "# initialize the train, validation and test dataloaders\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "\n",
        "# calculate steps per epoch for training, validation set\n",
        "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
        "valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
        "testSteps = len(testDataLoader.dataset) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "espa39G3PHxX",
        "outputId": "50f84ff9-9a65-4023-8dad-874a6d5f44bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] initializing the ResNet model...\n",
            "[INFO] training the network...\n"
          ]
        }
      ],
      "source": [
        "# load from drive previous results\n",
        "previous_state = torch.load(f\"{models_dir}/train_state_dict_3.pt\")\n",
        "\n",
        "# initialize the ResNet model\n",
        "print(\"[INFO] initializing the ResNet model...\")\n",
        "model = ResNet(block=block, numChannels=3, outputNodes=4).to(device)\n",
        "model.load_state_dict(previous_state['model_state_dict'])\n",
        "\n",
        "# initialize the optimizer and lr_scheduler\n",
        "opt = Adam(model.parameters(), lr=INIT_LR)\n",
        "opt.load_state_dict(previous_state['optimizer_state_dict'])\n",
        "\n",
        "scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=2)\n",
        "scheduler.load_state_dict(previous_state['scheduler_state_dict'])\n",
        "\n",
        "# initialize loss function and accuracy measurement\n",
        "lossFn = nn.MSELoss()\n",
        "r2score_metric = R2Score(multioutput='raw_values', device=device)\n",
        "\n",
        "# initialize a dictionary to store training and evaluation history\n",
        "H = {\n",
        "    \"magnitude_train_loss\":[],\n",
        "    \"latitude_train_loss\":[],\n",
        "    \"longitude_train_loss\":[],\n",
        "    \"depth_train_loss\":[],\n",
        "    \"magnitude_val_loss\":[],\n",
        "    \"latitude_val_loss\":[],\n",
        "    \"longitude_val_loss\":[],\n",
        "    \"depth_val_loss\":[],\n",
        "}\n",
        "\n",
        "# load previous loss history and last epoch\n",
        "H = previous_state['train_loss_history']\n",
        "last_epoch = previous_state['epoch']+1\n",
        "# measure how long training is going to take\n",
        "print(\"[INFO] training the network...\")\n",
        "startTime = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bcz3bGOU-En",
        "outputId": "728e9055-9218-4bf4-ea97-6303b2bc5f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] EPOCH: 13/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.58257, 0.02870 , 0.02337 , 0.05071, 2.22750\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.00940, 0.02323 , 0.02312 , 0.04329, 3.94798\n",
            "[INFO] In EPOCH 13 the loss value did not improve from 1.00210. This is the 1 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 14/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.52979, 0.02973 , 0.02365 , 0.05393, 2.01187\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.04500, 0.02454 , 0.02718 , 0.08215, 4.04611\n",
            "[INFO] In EPOCH 14 the loss value did not improve from 1.00210. This is the 2 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 15/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.47669, 0.02950 , 0.02252 , 0.05046, 1.80427\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.09792, 0.02427 , 0.04290 , 0.33768, 3.98683\n",
            "[INFO] In EPOCH 15 the loss value did not improve from 1.00210. This is the 3 EPOCH in a row.\n",
            "===========================================================================================\n",
            "[INFO] EPOCH: 16/50 ...\n",
            "Train loss (General, Magnitude, Latitude, Longitude, Depth): 0.36166, 0.02454 , 0.01847 , 0.03123, 1.37242\n",
            "Val loss (General, Magnitude, Latitude, Longitude, Depth): 1.02363, 0.02285 , 0.02003 , 0.05379, 3.99783\n",
            "[INFO] In EPOCH 16 the loss value did not improve from 1.00210. This is the 4 EPOCH in a row.\n",
            "[INFO] Early Stopping the train process. The patience has been exceeded!\n",
            "===========================================================================================\n",
            "[INFO] Total time taken to train the model: 2639.65s\n",
            "[INFO] The best loss value was found in EPOCH 12 where the performance was 1.00210. Model's parameters saved!\n"
          ]
        }
      ],
      "source": [
        "# loop over training epochs\n",
        "\n",
        "early_stopper = EarlyStopping(patience = 4)\n",
        "early_stopper.setBestLosses([0.63514, 0.03042 , 0.02362 , 0.05065, 2.43586],[1.00210, 0.02329 , 0.02095 , 0.07678, 3.88739])\n",
        "early_stopper.setBestEpoch(12)\n",
        "early_stopper.setCounter(0)\n",
        "early_stopper.setMinValLoss(1.00210)\n",
        "\n",
        "for e in range(last_epoch, EPOCHS):\n",
        "  # set the model in training model\n",
        "  model.train()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  magnitudeTrainLoss = 0\n",
        "  latitudeTrainLoss = 0\n",
        "  longitudeTrainLoss = 0\n",
        "  depthTrainLoss = 0\n",
        "  generalTrainLoss = 0\n",
        "\n",
        "  magnitudeValLoss = 0\n",
        "  latitudeValLoss = 0\n",
        "  longitudeValLoss = 0\n",
        "  depthValLoss = 0\n",
        "  generalValLoss = 0\n",
        "\n",
        "  for sampled_batch in trainDataLoader:\n",
        "    if sampled_batch is None:\n",
        "      continue\n",
        "\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "    # perform a forward pass and calculate training loss\n",
        "    pred = model(x)\n",
        "    loss = lossFn(pred, y)\n",
        "    # zero out the gradients, perfrom backprop step and update weights\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # add the loss to the total training so far\n",
        "    generalTrainLoss += loss.cpu().detach().numpy()\n",
        "    magnitudeTrainLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTrainLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTrainLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "    depthTrainLoss += lossFn(pred[:,3],y[:,3]).cpu().detach().numpy()\n",
        "\n",
        "  # switch off autograd for evaluation\n",
        "  with torch.no_grad():\n",
        "    # set the model evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # for loop over de validation set\n",
        "    for sampled_batch in valDataLoader:\n",
        "      if sampled_batch is None:\n",
        "        continue\n",
        "\n",
        "      # send the input to the device\n",
        "      (x, y) = (sampled_batch['spectrograms'].to(device), sampled_batch['results'].to(device))\n",
        "\n",
        "      # make the predictions and calculate valdiation loss\n",
        "      pred = model(x)\n",
        "      generalValLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "      magnitudeValLoss += lossFn(pred[:,0],y[:,0]).cpu().detach().numpy()\n",
        "      latitudeValLoss += lossFn(pred[:,1],y[:,1]).cpu().detach().numpy()\n",
        "      longitudeValLoss += lossFn(pred[:,2],y[:,2]).cpu().detach().numpy()\n",
        "      depthValLoss += lossFn(pred[:,3],y[:,3]).cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  # calculate the average training and validation loss\n",
        "  avgGeneralTrainLoss = generalTrainLoss / trainSteps\n",
        "  avgMagnitudeTrainLoss =  magnitudeTrainLoss / trainSteps\n",
        "  avgLatitudeTrainLoss =  latitudeTrainLoss / trainSteps\n",
        "  avgLongitudeTrainLoss =  longitudeTrainLoss / trainSteps\n",
        "  avgDepthTrainLoss =  depthTrainLoss / trainSteps\n",
        "\n",
        "  avgGeneralValLoss = generalValLoss / valSteps\n",
        "  avgMagnitudeValLoss =  magnitudeValLoss / valSteps\n",
        "  avgLatitudeValLoss =  latitudeValLoss / valSteps\n",
        "  avgLongitudeValLoss =  longitudeValLoss / valSteps\n",
        "  avgDepthValLoss =  depthValLoss / valSteps\n",
        "\n",
        "  # Adding another step through an epoch to the scheduler\n",
        "  scheduler.step(avgGeneralValLoss)\n",
        "\n",
        "  # update training and evaluation history\n",
        "  H[\"magnitude_train_loss\"].append(avgMagnitudeTrainLoss)\n",
        "  H[\"latitude_train_loss\"].append(avgLatitudeTrainLoss)\n",
        "  H[\"longitude_train_loss\"].append(avgLongitudeTrainLoss)\n",
        "  H[\"depth_train_loss\"].append(avgDepthTrainLoss)\n",
        "\n",
        "\n",
        "  H[\"magnitude_val_loss\"].append(avgMagnitudeValLoss)\n",
        "  H[\"latitude_val_loss\"].append(avgLatitudeValLoss)\n",
        "  H[\"longitude_val_loss\"].append(avgLongitudeValLoss)\n",
        "  H[\"depth_val_loss\"].append(avgDepthValLoss)\n",
        "\n",
        "  # saving current state dicts of the epoch and loss history - checkpoint\n",
        "  torch.save({\n",
        "            'epoch': e,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss_history': H\n",
        "            }, f\"{models_dir}/train_state_dict_3.pt\")\n",
        "\n",
        "  # print the model training and validation information\n",
        "  print(\"[INFO] EPOCH: {}/{} ...\".format(e+1, EPOCHS))\n",
        "  print(\"Train loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss, avgDepthTrainLoss))\n",
        "  print(\"Val loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss, avgDepthValLoss))\n",
        "\n",
        "  # checking if resulting loss in evaluation is the best\n",
        "  if early_stopper.earlyStop(avgGeneralValLoss, (e+1), [avgGeneralTrainLoss, avgMagnitudeTrainLoss, avgLatitudeTrainLoss, avgLongitudeTrainLoss, avgDepthTrainLoss],\n",
        "                [avgGeneralValLoss, avgMagnitudeValLoss, avgLatitudeValLoss, avgLongitudeValLoss, avgDepthValLoss]):\n",
        "    # if it is not better - stopping train process\n",
        "    print(\"[INFO] Early Stopping the train process. The patience has been exceeded!\")\n",
        "    print(\"===========================================================================================\")\n",
        "    break\n",
        "\n",
        "  print(\"===========================================================================================\")\n",
        "\n",
        "# finish measuring how long training took\n",
        "endTime = time.time()\n",
        "print(\"[INFO] Total time taken to train the model: {:.2f}s\".format(endTime-startTime))\n",
        "print(\"[INFO] The best loss value was found in EPOCH {} where the performance was {:.5f}. Model's parameters saved!\".format(early_stopper.getBestEpoch(), early_stopper.getBestValLosses()[0]))\n",
        "early_stopper.saveLossesLocally()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rprn14X5Ksvh"
      },
      "outputs": [],
      "source": [
        "# plot the training and val losses\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# Plotting magnitude loss on train and evaluation\n",
        "plt.figure(\"magnitude_loss\").clear()\n",
        "plt.plot(H[\"magnitude_train_loss\"], label=\"magnitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"magnitude_val_loss\"], label=\"magnitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Magnitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_magnitude_loss.png\")\n",
        "\n",
        "# Plotting latitude loss on train and evaluation\n",
        "plt.figure(\"latitude_loss\").clear()\n",
        "plt.plot(H[\"latitude_train_loss\"], label=\"latitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"latitude_val_loss\"], label=\"latitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Latitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_latitude_loss.png\")\n",
        "\n",
        "# Plotting longitude loss on train and evaluation\n",
        "plt.figure(\"longitude_loss\").clear()\n",
        "plt.plot(H[\"longitude_train_loss\"], label=\"longitude_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"longitude_val_loss\"], label=\"longitude_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Longitude Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_longitude_loss.png\")\n",
        "\n",
        "# Plotting depth loss on train and evaluation\n",
        "plt.figure(\"depth_loss\").clear()\n",
        "plt.plot(H[\"depth_train_loss\"], label=\"depth_train_loss\", linestyle=\"solid\")\n",
        "plt.plot(H[\"depth_val_loss\"], label=\"depth_val_loss\", linestyle=\"solid\")\n",
        "plt.title(\"Depth Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_depth_loss.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HVoM4I-b9Wc",
        "outputId": "4f91f844-f2a3-4fad-abbb-ec4d14057525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "[INFO] Loss/Accuracy values obtained on the test set\n",
            "[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): 1.00401, 0.02468 , 0.02240 , 0.03391, 3.93505\n",
            "[INFO] R2 Score obtained on the test set: [0.9100773  0.71742815 0.5779206  0.8045777 ]\n"
          ]
        }
      ],
      "source": [
        "# evaluation of the network in the test set\n",
        "print(\"[INFO] evaluating network...\")\n",
        "r2_score = R2Score()\n",
        "model.load_state_dict(torch.load(f\"{models_dir}/ResNet_state_dict.pt\"))\n",
        "\n",
        "test_results = {\n",
        "    \"true_values\":[],\n",
        "    \"pred_values\":[]\n",
        "}\n",
        "\n",
        "# turn off autograd for testing evaluation\n",
        "with torch.no_grad():\n",
        "  # set the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # initialize the total training validation loss\n",
        "  generalTestLoss = 0\n",
        "  magnitudeTestLoss = 0\n",
        "  latitudeTestLoss =  0\n",
        "  longitudeTestLoss = 0\n",
        "  depthTestLoss = 0\n",
        "\n",
        "  # loop over the test set\n",
        "  for sampled_batch in testDataLoader:\n",
        "    if sampled_batch is None:\n",
        "      continue\n",
        "\n",
        "    # send the input to the device\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device),sampled_batch['results'].to(device))\n",
        "    test_results[\"true_values\"].append(y.cpu().detach().numpy().tolist())\n",
        "\n",
        "    # make the predictions and add them to the list\n",
        "    pred = model(x)\n",
        "    test_results[\"pred_values\"].append(pred.cpu().detach().numpy().tolist())\n",
        "\n",
        "    generalTestLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "    magnitudeTestLoss += lossFn(pred[:,0], y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTestLoss += lossFn(pred[:,1], y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTestLoss += lossFn(pred[:,2], y[:,2]).cpu().detach().numpy()\n",
        "    depthTestLoss += lossFn(pred[:,3], y[:,3]).cpu().detach().numpy()\n",
        "    r2score_metric.update(pred, y)\n",
        "\n",
        "  # generate test MSE and R2 Score\n",
        "  avgGeneralTestLoss = generalTestLoss / testSteps\n",
        "  avgMagnitudeTestLoss = magnitudeTestLoss / testSteps\n",
        "  avgLatitudeTestLoss =  latitudeTestLoss / testSteps\n",
        "  avgLongitudeTestLoss = longitudeTestLoss / testSteps\n",
        "  avgDepthTestLoss =  depthTestLoss / testSteps\n",
        "\n",
        "  r2score_value = r2score_metric.compute()\n",
        "\n",
        "  print(\"[INFO] Loss/Accuracy values obtained on the test set\")\n",
        "  print(\"[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralTestLoss, avgMagnitudeTestLoss, avgLatitudeTestLoss, avgLongitudeTestLoss, avgDepthTestLoss))\n",
        "  print(\"[INFO] R2 Score obtained on the test set: {}\".format(r2score_value.cpu().detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LeOoMEdVd0nR",
        "outputId": "51acf481-006a-4b34-8014-8b85e55288ec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ffda9087b731>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# generating table with best values obtained on train, evaluation and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mearly_stopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadLossesLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mmodels_performance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# used to showcase the Loss/Accuracy values obtained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'early_stopper' is not defined"
          ]
        }
      ],
      "source": [
        "# plotting and saving plots for Y_true - Y_pred\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "\"\"\"\n",
        "puncte = torch.load(f\"{models_dir}/true_pred_points.pt\")\n",
        "\"\"\"\n",
        "test_true = []\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(test_results[\"true_values\"])):\n",
        "  test_true.extend(test_results[\"true_values\"][i])\n",
        "  test_pred.extend(test_results[\"pred_values\"][i])\n",
        "\n",
        "test_true = np.array(test_true)\n",
        "test_pred = np.array(test_pred)\n",
        "\n",
        "\n",
        "torch.save({\n",
        "            'true_points':test_true,\n",
        "            'pred_points':test_pred\n",
        "            }, f\"{models_dir}/true_pred_points_resnet3.pt\")\n",
        "\n",
        "\n",
        "# Plotting magnitude test Y_true - Y_pred\n",
        "plt.figure(\"magnitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,0], test_pred[:,0], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,0], test_pred[:,0], 1)\n",
        "plt.plot(test_true[:,0], m*test_true[:,0]+b,\"--r\")\n",
        "plt.title(\"Magnitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_magnitude_true-pred.png\")\n",
        "\n",
        "# Plotting latitude test Y_true - Y_pred\n",
        "plt.figure(\"latitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,1], test_pred[:,1], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,1], test_pred[:,1], 1)\n",
        "plt.plot(test_true[:,1], m*test_true[:,1]+b,\"--r\")\n",
        "plt.title(\"Latitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_latitude_true-pred.png\")\n",
        "\n",
        "# Plotting longitude test Y_true - Y_pred\n",
        "plt.figure(\"longitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,2], test_pred[:,2], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,2], test_pred[:,2], 1)\n",
        "plt.plot(test_true[:,2], m*test_true[:,2]+b,\"--r\")\n",
        "plt.title(\"Longitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_longitude_true-pred.png\")\n",
        "\n",
        "# Plotting depth test Y_true - Y_pred\n",
        "plt.figure(\"depth_true-pred\").clear()\n",
        "plt.plot(test_true[:,3], test_pred[:,3], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,3], test_pred[:,3], 1)\n",
        "plt.plot(test_true[:,3], m*test_true[:,3]+b,\"--r\")\n",
        "plt.title(\"Adancime Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_depth_true-pred.png\")\n",
        "\n",
        "\n",
        "# generating table with best values obtained on train, evaluation and test\n",
        "early_stopper.loadLossesLocally()\n",
        "\n",
        "models_performance = [] # used to showcase the Loss/Accuracy values obtained\n",
        "models_performance.append([\"General MSE\", round(early_stopper.getBestTrainLosses()[0], 5),  round(early_stopper.getBestValLosses()[0], 5), avgGeneralTestLoss])\n",
        "models_performance.append([\"Magnitude MSE\", round(early_stopper.getBestTrainLosses()[1], 5),  round(early_stopper.getBestValLosses()[1], 5), avgMagnitudeTestLoss])\n",
        "models_performance.append([\"Latitude MSE\", round(early_stopper.getBestTrainLosses()[2], 5),  round(early_stopper.getBestValLosses()[2], 5), avgLatitudeTestLoss])\n",
        "models_performance.append([\"Longitude MSE\", round(early_stopper.getBestTrainLosses()[3], 5),  round(early_stopper.getBestValLosses()[3], 5), avgLongitudeTestLoss])\n",
        "models_performance.append([\"Depth MSE\", round(early_stopper.getBestTrainLosses()[4], 5),  round(early_stopper.getBestValLosses()[4], 5), avgDepthTestLoss])\n",
        "\n",
        "r2_score_list = r2score_value.cpu().detach().numpy().tolist()\n",
        "models_performance.append([\"R2 Score\", r2_score_list[0], r2_score_list[1], r2_score_list[2], r2_score_list[3]])\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Loss obtained on Train, Valdation and Test sets\")\n",
        "print(tabulate(models_performance[0:5], headers=[\"Metric\", \"Train\", \"Validation\", \"Test\"], tablefmt=\"github\"))\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Accuracy obtained on the Test set\")\n",
        "print(tabulate([models_performance[5]], headers=[\"Metric\", \"Magnitude\", \"Latitude\", \"Longitude\", \"Depth\"], tablefmt=\"github\"))\n",
        "\n",
        "# serialize the model to disk\n",
        "torch.save(model, f\"{models_dir}/ResNet3_model.pt\")\n",
        "\n",
        "# TODO: Justify hop_length, batch size\n",
        "# TODO: Covariate shift and vanishing gradients\n",
        "\n",
        "\n",
        "# De ce am loss mare pe train in prima epoca, indiferent de LR?\n",
        "# Regularization - weight decay?\n",
        "# Cum plotez dreapta de regresie pentru norul de puncte - numpy ployfit?\n",
        "# Folosesc plotly ca sa creez harta interactiva sau pastrez versiunea cu Basemap?\n",
        "\n",
        "# TODO: BatchNorm(and ResNet) - Caci din ce vad, eu am un plain CNN\n",
        "\n",
        "# Concluzii rezultate obținute.\n",
        "# Cum justific alegerea structurii retelei neuronale?\n",
        "# Ma asteptam sa fac eu ceva de la 0 sau sa modific la o structura existenta,\n",
        "# doar ca nu inteleg cum as putea justifica alegerea facuta?\n",
        "# De invatat mai multe despre ResNets\n",
        "\n",
        "# Cum justific alegerea batch size, epochs si hop_length?\n",
        "# De cautat\n",
        "\n",
        "# Tinand cont ca datasetul este de 41365, ar fi bine sa iau 41360 pentru a fi\n",
        "# exacta impartirea 70% - 15% - 15%? Sau nu are relevanta\n",
        "# Nu\n",
        "\n",
        "# Este ok layer ul final adaugat de mine? (MLP (flattent)-2048-1024-3)\n",
        "# Da\n",
        "\n",
        "# Folosesc tot setul de date sau iau mai putin? Daca iau tot setul de date, ar fi\n",
        "# ok sa iau si un batch size mai mare?\n",
        "# Nu accelereaza antrenarea. Setul de date e perfect asa cum este. Teste pe 10000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "maps_dir = '/content/drive/My Drive/Map'\n",
        "# Define the data for the ground truth and predicted epicenters\n",
        "gt_magnitudes = test_true[:,0]\n",
        "gt_latitudes = test_true[:,1]\n",
        "gt_longitudes = test_true[:,2]\n",
        "gt_depth = test_true[:,3]\n",
        "\n",
        "pred_magnitudes = test_pred[:,0]\n",
        "pred_latitudes = test_pred[:,1]\n",
        "pred_longitudes = test_pred[:,2]\n",
        "pred_depth = test_pred[:,3]\n",
        "\n",
        "# Define the layout for the map\n",
        "layout = go.Layout(\n",
        "    mapbox=dict(\n",
        "        center=dict(lat=33.5, lon=-116.8),\n",
        "        zoom=8,\n",
        "        style='stamen-terrain'\n",
        "    ),\n",
        "    title='Ground Truth and Predicted Epicenters')\n",
        "\n",
        "\n",
        "# Define the hover text for the markers\n",
        "gt_hover_text = ['Numar cutremur: {} <br>Latitudine: {}<br>Longitudine: {} <br>Magnitudine: {} <br>Adancime: {}'.format(num, lat, lon, magn, depth)\n",
        "                 for num, (lat, lon, magn, depth) in enumerate(zip(gt_latitudes, gt_longitudes, gt_magnitudes, gt_depth))]\n",
        "pred_hover_text = ['Numar cutremur: {} <br>Latitudine: {}<br>Longitudine: {} <br>Magnitudine: {} <br>Adancime: {}'.format(num, lat, lon, magn, depth)\n",
        "                   for num, (lat, lon, magn, depth) in enumerate(zip(pred_latitudes, pred_longitudes, pred_magnitudes, pred_depth))]\n",
        "\n",
        "# Create the scatter mapbox trace for the ground truth and predicted epicenters\n",
        "gt_trace = go.Scattermapbox(\n",
        "    lat=gt_latitudes,\n",
        "    lon=gt_longitudes,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color='blue'\n",
        "    ),\n",
        "    name='Ground Truth',\n",
        "    hovertext=gt_hover_text,\n",
        "    hoverinfo='text'\n",
        ")\n",
        "\n",
        "pred_trace = go.Scattermapbox(\n",
        "    lat=pred_latitudes,\n",
        "    lon=pred_longitudes,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color='red'\n",
        "    ),\n",
        "    name='Predicted',\n",
        "    hovertext=pred_hover_text,\n",
        "    hoverinfo='text'\n",
        ")\n",
        "\n",
        "# Create the figure and add the traces and layout\n",
        "fig = go.Figure(data=[gt_trace, pred_trace], layout=layout)\n",
        "\n",
        "# Show the figure\n",
        "fig.write_html(f\"{maps_dir}/Test2_ResNet3_map.html\")"
      ],
      "metadata": {
        "id": "1ApCT5Zsy3KF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}