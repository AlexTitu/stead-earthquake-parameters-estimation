{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8k8xwh83ZJK",
        "outputId": "8f7d55fe-7a1f-4dec-e09e-315c5fe8894f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.6-py3-none-any.whl (158 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtnt>=0.0.5 (from torcheval)\n",
            "  Downloading torchtnt-0.1.0-py3-none-any.whl (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (1.22.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2023.6.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (2.12.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (5.9.5)\n",
            "Collecting pyre-extensions (from torchtnt>=0.0.5->torcheval)\n",
            "  Downloading pyre_extensions-0.0.30-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtnt>=0.0.5->torcheval) (4.65.0)\n",
            "Collecting typing-inspect (from pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.4.3)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->torchtnt>=0.0.5->torcheval) (0.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtnt>=0.0.5->torcheval) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtnt>=0.0.5->torcheval) (16.0.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->torchtnt>=0.0.5->torcheval) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->torchtnt>=0.0.5->torcheval) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtnt>=0.0.5->torcheval) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions->torchtnt>=0.0.5->torcheval)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->torchtnt>=0.0.5->torcheval) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->torchtnt>=0.0.5->torcheval) (3.2.2)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyre-extensions, torchtnt, torcheval\n",
            "Successfully installed mypy-extensions-1.0.0 pyre-extensions-0.0.30 torcheval-0.0.6 torchtnt-0.1.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.8.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEzsjsCZ3OaN",
        "outputId": "f100fa7d-b675-4fdf-e40d-14b86b3ac8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "import h5py\n",
        "import numpy as np\n",
        "import librosa as lib\n",
        "import librosa.display as libd\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import copy\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "plots_dir = '/content/drive/My Drive/Plots/full'\n",
        "models_dir = '/content/drive/My Drive/Models/full'\n",
        "maps_dir = '/content/drive/My Drive/Map'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-70OKCLZ6C2Y"
      },
      "outputs": [],
      "source": [
        "class STEADDataset(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file,hdf5_file,transform=None):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        csv_file (string): calea catre fisierul csv cu etichete.\n",
        "        hdf5_file (string): fisierul cu formele de unda.\n",
        "        transform (callable, optional): transformare a unui esantion.\n",
        "    \"\"\"\n",
        "    self.tags = pd.read_csv(csv_file)\n",
        "    self.hdf5_file = hdf5_file\n",
        "    self.traces = self.tags['trace_name'].to_list()\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.traces)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "\n",
        "    dataset = h5py.File(self.hdf5_file,'r')\n",
        "    tracename = self.traces[idx]\n",
        "    waveform = dataset.get('data/'+tracename)\n",
        "    data = np.array(waveform)\n",
        "    spectrograms = self.getSpectrogram(data)\n",
        "\n",
        "    sample = {'spectrograms':spectrograms,\n",
        "              'source_magnitude':waveform.attrs['source_magnitude'],\n",
        "              'source_latitude':waveform.attrs['source_latitude'],\n",
        "              'source_longitude':waveform.attrs['source_longitude'],\n",
        "              'source_depth_km':waveform.attrs['source_depth_km']}\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    dataset.close()\n",
        "    return sample\n",
        "\n",
        "  def getSpectrogram(self, waveforms):\n",
        "    # defining axis\n",
        "    EW = waveforms[:,0]\n",
        "    NS = waveforms[:,1]\n",
        "    Vert = waveforms[:,2]\n",
        "\n",
        "    EW_ft = lib.stft(EW,n_fft=1024, hop_length=32)\n",
        "    NS_ft = lib.stft(NS,n_fft=1024, hop_length=32)\n",
        "    Vert_ft = lib.stft(Vert,n_fft=1024, hop_length=32)\n",
        "\n",
        "    EW_db = lib.amplitude_to_db(np.abs(EW_ft), ref=np.max)\n",
        "    NS_db = lib.amplitude_to_db(np.abs(NS_ft), ref=np.max)\n",
        "    Vert_db = lib.amplitude_to_db(np.abs(Vert_ft), ref=np.max)\n",
        "\n",
        "    spectrograms = np.array([EW_db, NS_db, Vert_db])\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Converteste NumPY ndarrays la Tensori.\"\"\"\n",
        "\n",
        "    def __call__(self, sample):\n",
        "      if sample['source_depth_km'] == \"None\":\n",
        "        return None\n",
        "      else:\n",
        "        spectrograms = sample['spectrograms']\n",
        "        results = np.array([sample['source_magnitude'],\n",
        "                            sample['source_latitude'],\n",
        "                            sample['source_longitude'],\n",
        "                            sample['source_depth_km']], dtype=np.float32)\n",
        "\n",
        "        return {'spectrograms': torch.from_numpy(spectrograms),\n",
        "                'results':  torch.from_numpy(results)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs8di3641aYp"
      },
      "outputs": [],
      "source": [
        "def custom_collate_fn(batch):\n",
        "  # filtrez estanioanele None (regasite in cazul etichetei adancimii)\n",
        "  filtered_batch = [sample for sample in batch if sample is not None]\n",
        "  if len(filtered_batch) == 0:\n",
        "    # daca lotul nu are niciun esantion – toate None\n",
        "    return None\n",
        "  else:\n",
        "    # creez noul batch cu esantioanele None eliminate\n",
        "    return default_collate(filtered_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VIF677lV3bK",
        "outputId": "ac2c4207-c3b2-478b-ee70-266814d19598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-c971ea9a199e>:10: DtypeWarning:\n",
            "\n",
            "Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "STEAD_dataset = STEADDataset(csv_file=\"drive/My Drive/dataset.csv\",\n",
        "                             hdf5_file=\"drive/My Drive/dataset.hdf5\",\n",
        "                             transform=ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y46scF2YAp65"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module # Implementez o clasa in locul unui model secvential\n",
        "from torch.nn import Conv2d # strat convolutional\n",
        "from torch.nn import BatchNorm2d # Normalizarea loturilor\n",
        "from torch.nn import Linear # functia de regresie liniara\n",
        "from torch.nn import MaxPool2d # esantionare 2D - MaxPooling\n",
        "from torch.nn import ReLU # functie de activare\n",
        "from torch.optim import Adam # Optimizator Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # programator al ratei de invatare\n",
        "from torch import flatten # redimensionarea in start fc 1D\n",
        "from torch.utils.data import random_split # pentru impartirea dataset-ului\n",
        "from torcheval.metrics import R2Score # evalueaza precizia modelului\n",
        "from torch import nn\n",
        "import torch\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFp058dTkagm"
      },
      "outputs": [],
      "source": [
        "# definesc blocul rezidual\n",
        "class block(Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=(5,5),\n",
        "               padding=(2,2)):\n",
        "    super(block, self).__init__()\n",
        "    self.conv1 = Conv2d(in_channels, out_channels,\n",
        "                        kernel_size = kernel_size, padding=padding)\n",
        "    self.bn1 = BatchNorm2d(out_channels)\n",
        "    self.conv2 = Conv2d(out_channels, out_channels,\n",
        "                        kernel_size= kernel_size, padding=padding)\n",
        "    self.bn2 = BatchNorm2d(out_channels)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    identity = x\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x += identity\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    output = self.maxpool(x)\n",
        "\n",
        "    return output\n",
        "\n",
        "class ResNet(Module):\n",
        "  def __init__(self, block, numChannels, outputNodes):\n",
        "    super(ResNet, self).__init__()\n",
        "    # initializez primul strat CONV => RELU => POOL\n",
        "    self.conv1 = Conv2d(in_channels=numChannels, out_channels=16,\n",
        "                        kernel_size=(7,7), padding=(3,3))\n",
        "    self.bn1 = BatchNorm2d(16)\n",
        "    self.relu = ReLU(inplace=True)\n",
        "    self.maxpool = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
        "\n",
        "    # initializez al doilea strat CONV => RELU => POOL\n",
        "    self.conv2 = Conv2d(in_channels=16, out_channels=16, kernel_size=(7,7),\n",
        "                        padding=(3,3))\n",
        "    self.bn2 = BatchNorm2d(16)\n",
        "\n",
        "    # straturile de tip ResNet\n",
        "    self.layer1 = block(in_channels=16, out_channels=32, kernel_size=(5,5),\n",
        "                        padding=(2,2))\n",
        "    self.layer2 = block(in_channels=32, out_channels=64, kernel_size=(3,3),\n",
        "                        padding=(1,1))\n",
        "    self.layer3 = block(in_channels=64, out_channels=96, kernel_size=(3,3),\n",
        "                        padding=(1,1))\n",
        "\n",
        "    # initializez ultimul strat CONV => RELU => CONV => POOL\n",
        "    self.conv3 = Conv2d(in_channels=96, out_channels=128, kernel_size=(3,3),\n",
        "                        padding=(1,1))\n",
        "    self.bn3 = BatchNorm2d(128)\n",
        "\n",
        "    self.conv4 = Conv2d(in_channels=128, out_channels=128,\n",
        "                        kernel_size=(3,3), padding=(1,1))\n",
        "    self.bn4 = BatchNorm2d(128)\n",
        "\n",
        "    # initializez stratul FC => ReLU layers - fully connected\n",
        "    self.fc1 = Linear(in_features=2048, out_features=1024)\n",
        "\n",
        "    # initializez stratul FC => Linear *Regression*\n",
        "    self.fc2 = Linear(in_features=1024, out_features=outputNodes)\n",
        "\n",
        "  def forward(self, x):\n",
        "  # trimit intrarea prin primul bloc CONV -> ReLU -> POOL\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # trimit rezultatele prin al doilea bloc CONV -> ReLU -> POOL\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # trimit datele prin blocurile reziduale\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "\n",
        "    # trimit datele prin blocul CONV => RELU => CONV => POOL\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.bn4(x)\n",
        "    x = self.maxpool(x)\n",
        "\n",
        "    # redimensionez iesirea stratului anterior si trimit catre stratul ascuns FC\n",
        "    x = flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    # trimit datele catre stratul Linear pentru realizarea regresiei\n",
        "    output = self.fc2(x)\n",
        "\n",
        "    # intorc rezultatele prezise\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "# definesc clasa EarlyStopping\n",
        "class EarlyStopping():\n",
        "  def __init__(self, patience = 1, min_delta = 0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.min_validation_loss = np.inf\n",
        "    self.best_epoch = 0\n",
        "    self.best_train = [None] * 5\n",
        "    self.best_val = [None] * 5\n",
        "\n",
        "  def earlyStop(self, validation_loss, epoch, TrainLoss, ValLoss):\n",
        "    if validation_loss <= self.min_validation_loss:\n",
        "      print(\"[INFO] In EPOCH {} the loss value improved from {:.5f} to {:.5f}\".format(epoch, self.min_validation_loss, validation_loss))\n",
        "      self.min_validation_loss = validation_loss\n",
        "      self.counter = 0\n",
        "      self.best_epoch = epoch\n",
        "# salvez ponderile celui mai performant model de pana acum\n",
        "      torch.save(model.state_dict(), f\"{models_dir}/ResNet_state_dict.pt\")\n",
        "      self.setBestLosses(TrainLoss, ValLoss)\n",
        "\n",
        "    elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "      self.counter += 1\n",
        "      print(\"[INFO] In EPOCH {} the loss value did not improve from {:.5f}. This is the {} EPOCH in a row.\".format(epoch, self.min_validation_loss,\n",
        "                                  self.counter))\n",
        "      if self.counter >= self.patience:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  def setCounter(self, counter_state):\n",
        "    self.counter = counter_state\n",
        "\n",
        "  def setMinValLoss(self, ValLoss):\n",
        "    self.min_validation_loss = ValLoss\n",
        "\n",
        "  def setBestLosses(self, TrainLoss, ValLoss):\n",
        "    self.best_train = TrainLoss\n",
        "    self.best_val = ValLoss\n",
        "\n",
        "  def setBestEpoch(self, bestEpoch):\n",
        "    self.best_epoch = bestEpoch\n",
        "\n",
        "  def getBestTrainLosses(self):\n",
        "    return self.best_train\n",
        "\n",
        "  def getBestValLosses(self):\n",
        "    return self.best_val\n",
        "\n",
        "  def getBestEpoch(self):\n",
        "    return self.best_epoch\n",
        "\n",
        "  def saveLossesLocally(self):\n",
        "    np.save(f'{models_dir}/losses_train.npy', np.array(self.best_train))\n",
        "    np.save(f'{models_dir}/losses_val.npy', np.array(self.best_val))\n",
        "\n",
        "  def loadLossesLocally(self):\n",
        "    self.best_train = np.load(f'{models_dir}/losses_train.npy')\n",
        "    self.best_val = np.load(f'{models_dir}/losses_val.npy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWaYTXUM_Ejs",
        "outputId": "e070dc97-4961-449b-c33a-7deceb155232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] device used for training...cuda\n"
          ]
        }
      ],
      "source": [
        "# definesc hiperparametrii\n",
        "INIT_LR = 1*1e-4\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 50\n",
        "\n",
        "# definesc impartirea set de antrenare-validare-testare\n",
        "VAL_TEST_SPLIT = 0.06/100\n",
        "TRAIN_SPLIT = 1 - VAL_TEST_SPLIT\n",
        "\n",
        "# aleg dispozitivul pe care vor avea loc operatiile\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"[INFO] device used for training...{}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmQSr35LAfSk",
        "outputId": "51d02ffb-4d45-4367-c03c-2a348d17babe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] generating the train/validation split...\n"
          ]
        }
      ],
      "source": [
        "# calculez numarul de esantioane pentru fiecare set de date\n",
        "print(\"[INFO] generating the train/validation split...\")\n",
        "numTrainSamples = int(len(STEAD_dataset)*TRAIN_SPLIT)\n",
        "# numValSamples = int(len(STEAD_dataset)*VAL_TEST_SPLIT)\n",
        "numTestSamples = int(len(STEAD_dataset)-(numTrainSamples))\n",
        "# fac impartirea aleatoare a esantioanelor\n",
        "(trainData, testData) = random_split(STEAD_dataset,[numTrainSamples, numTestSamples], generator=torch.Generator().manual_seed(19))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpR-V_0qNCUW"
      },
      "outputs": [],
      "source": [
        "# initializez setul de antrenare, validare (comentat), testare\n",
        "trainDataLoader = DataLoader(trainData, shuffle=True, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "# valDataLoader = DataLoader(valData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "testDataLoader = DataLoader(testData, batch_size=BATCH_SIZE, collate_fn = custom_collate_fn)\n",
        "\n",
        "# calculez numarul de pasi in functie de dimensiunea lotului de date de intrare\n",
        "trainSteps = len(trainDataLoader.dataset) // BATCH_SIZE\n",
        "# valSteps = len(valDataLoader.dataset) // BATCH_SIZE\n",
        "testSteps = len(testDataLoader.dataset) // BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "espa39G3PHxX",
        "outputId": "39d1ec91-99ba-4f3d-9369-0419fea732a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] initializing the ResNet model...\n",
            "[INFO] evaluating network...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# initializez modelul ResNet-10\n",
        "print(\"[INFO] initializing the ResNet model...\")\n",
        "model = ResNet(block=block, numChannels=3, outputNodes=4).to(device)\n",
        "\n",
        "# initializez functia de pierdere si coeficientul de determinare R2\n",
        "lossFn = nn.MSELoss()\n",
        "r2score_metric = R2Score(multioutput='raw_values', device=device)\n",
        "\n",
        "# incarc ponderile si incep evaluarea retelei neurale\n",
        "print(\"[INFO] evaluating network...\")\n",
        "model.load_state_dict(torch.load(f\"{models_dir}/ResNet_state_dict.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HVoM4I-b9Wc",
        "outputId": "440785db-4ac5-414f-cbba-f1a8ed5c8387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loss/Accuracy values obtained on the test set\n",
            "[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): 0.81112, 0.05462 , 0.02166 , 0.03019, 3.13801\n",
            "[INFO] R2 Score obtained on the test set: [0.68294966 0.7292813  0.57623273 0.78304136]\n"
          ]
        }
      ],
      "source": [
        "test_results = {\n",
        "    \"true_values\":[],\n",
        "    \"pred_values\":[]\n",
        "}\n",
        "\n",
        "# opresc autograd pentru testarea retelei\n",
        "with torch.no_grad():\n",
        "  # setez modeleul in modul evaluare\n",
        "  model.eval()\n",
        "\n",
        "  # initializez costurile totale pe setul de test\n",
        "  generalTestLoss = 0\n",
        "  magnitudeTestLoss = 0\n",
        "  latitudeTestLoss =  0\n",
        "  longitudeTestLoss = 0\n",
        "  depthTestLoss = 0\n",
        "\n",
        "  # interez prin setul de test\n",
        "  for sampled_batch in testDataLoader:\n",
        "    if sampled_batch is None:\n",
        "      continue\n",
        "\n",
        "    # trimit datele de intrare catre dispozitiv\n",
        "    (x, y) = (sampled_batch['spectrograms'].to(device),sampled_batch['results'].to(device))\n",
        "    test_results[\"true_values\"].append(y.cpu().detach().numpy().tolist())\n",
        "\n",
        "    # fac estimarile si le adaug in lista\n",
        "    pred = model(x)\n",
        "    test_results[\"pred_values\"].append(pred.cpu().detach().numpy().tolist())\n",
        "\n",
        "    generalTestLoss += lossFn(pred, y).cpu().detach().numpy()\n",
        "    magnitudeTestLoss += lossFn(pred[:,0], y[:,0]).cpu().detach().numpy()\n",
        "    latitudeTestLoss += lossFn(pred[:,1], y[:,1]).cpu().detach().numpy()\n",
        "    longitudeTestLoss += lossFn(pred[:,2], y[:,2]).cpu().detach().numpy()\n",
        "    depthTestLoss += lossFn(pred[:,3], y[:,3]).cpu().detach().numpy()\n",
        "    r2score_metric.update(pred, y)\n",
        "\n",
        "  # generez MSE si scorul R2 pe setul de test\n",
        "  avgGeneralTestLoss = generalTestLoss / testSteps\n",
        "  avgMagnitudeTestLoss = magnitudeTestLoss / testSteps\n",
        "  avgLatitudeTestLoss =  latitudeTestLoss / testSteps\n",
        "  avgLongitudeTestLoss = longitudeTestLoss / testSteps\n",
        "  avgDepthTestLoss =  depthTestLoss / testSteps\n",
        "\n",
        "  r2score_value = r2score_metric.compute()\n",
        "\n",
        "  print(\"[INFO] Loss/Accuracy values obtained on the test set\")\n",
        "  print(\"[INFO] Test loss (General, Magnitude, Latitude, Longitude, Depth): {:.5f}, {:.5f} , {:.5f} , {:.5f}, {:.5f}\".format(\n",
        "      avgGeneralTestLoss, avgMagnitudeTestLoss, avgLatitudeTestLoss, avgLongitudeTestLoss, avgDepthTestLoss))\n",
        "  print(\"[INFO] R2 Score obtained on the test set: {}\".format(r2score_value.cpu().detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeOoMEdVd0nR",
        "outputId": "3e004165-5bf0-4716-91d5-7d063f7073de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------\n",
            "Costurile obtinute pe loturile de antrenare, validare si testare\n",
            "| Metrică         |   Antrenare |   Validare |   Testare |\n",
            "|-----------------|-------------|------------|-----------|\n",
            "| MSE General     |     0.70137 |    1.11172 | 0.811121  |\n",
            "| MSE Magnitudine |     0.03283 |    0.03244 | 0.0546233 |\n",
            "| MSE Latitudine  |     0.03281 |    0.03493 | 0.0216575 |\n",
            "| MSE Longitudine |     0.04325 |    0.04592 | 0.0301934 |\n",
            "| MSE Adâncime    |     2.6966  |    4.33358 | 3.13801   |\n",
            "----------------------------------------------------------------------\n",
            "Precizia modelului pe setul de test\n",
            "| Metrică   |   Magnitudine |   Latitudine |   Longitudine |   Adâncime |\n",
            "|-----------|---------------|--------------|---------------|------------|\n",
            "| R2 Score  |       0.68295 |     0.729281 |      0.576233 |   0.783041 |\n"
          ]
        }
      ],
      "source": [
        "# realizez și salvez graficele de dispersie pentru Y_observat - Y_estimat\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "test_true = []\n",
        "test_pred = []\n",
        "\n",
        "for i in range(len(test_results[\"true_values\"])):\n",
        "  test_true.extend(test_results[\"true_values\"][i])\n",
        "  test_pred.extend(test_results[\"pred_values\"][i])\n",
        "\n",
        "test_true = np.array(test_true)\n",
        "test_pred = np.array(test_pred)\n",
        "\n",
        "# realizez graficul dispersiei punctelor de magnitudine Y_observat - Y_estimat\n",
        "plt.figure(\"magnitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,0], test_pred[:,0], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,0], test_pred[:,0], 1)\n",
        "plt.plot(test_true[:,0], m*test_true[:,0]+b,\"--r\")\n",
        "plt.title(\"Magnitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_magnitudine_obs-est.png\")\n",
        "\n",
        "# realizez graficul dispersiei punctelor de latitudine Y_observat - Y_estimat\n",
        "plt.figure(\"latitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,1], test_pred[:,1], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,1], test_pred[:,1], 1)\n",
        "plt.plot(test_true[:,1], m*test_true[:,1]+b,\"--r\")\n",
        "plt.title(\"Latitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_latitudine_obs-est.png\")\n",
        "\n",
        "# realizez graficul dispersiei punctelor de longitudine Y_observat - Y_estimat\n",
        "plt.figure(\"longitude_true-pred\").clear()\n",
        "plt.plot(test_true[:,2], test_pred[:,2], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,2], test_pred[:,2], 1)\n",
        "plt.plot(test_true[:,2], m*test_true[:,2]+b,\"--r\")\n",
        "plt.title(\"Longitudine Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_longitudine_obs-est.png\")\n",
        "\n",
        "# realizez graficul dispersiei punctelor de adancime Y_observat - Y_estimat\n",
        "plt.figure(\"depth_true-pred\").clear()\n",
        "plt.plot(test_true[:,3], test_pred[:,3], \"ob\")\n",
        "m, b = np.polyfit(test_true[:,3], test_pred[:,3], 1)\n",
        "plt.plot(test_true[:,3], m*test_true[:,3]+b,\"--r\")\n",
        "plt.title(\"Adancime Y_estimat vs Y_observat\")\n",
        "plt.xlabel(\"Y_observat\")\n",
        "plt.ylabel(\"Y_estimat\")\n",
        "plt.savefig(f\"{plots_dir}/ResNet3_adancime_obs-est.png\")\n",
        "\n",
        "\n",
        "# generating table with best values obtained on train, evaluation and test\n",
        "early_stopper = EarlyStopping(patience = 4)\n",
        "early_stopper.loadLossesLocally()\n",
        "\n",
        "models_performance = [] # used to showcase the Loss/Accuracy values obtained\n",
        "models_performance.append([\"MSE General\", round(early_stopper.getBestTrainLosses()[0], 5),  round(early_stopper.getBestValLosses()[0], 5), avgGeneralTestLoss])\n",
        "models_performance.append([\"MSE Magnitudine\", round(early_stopper.getBestTrainLosses()[1], 5),  round(early_stopper.getBestValLosses()[1], 5), avgMagnitudeTestLoss])\n",
        "models_performance.append([\"MSE Latitudine\", round(early_stopper.getBestTrainLosses()[2], 5),  round(early_stopper.getBestValLosses()[2], 5), avgLatitudeTestLoss])\n",
        "models_performance.append([\"MSE Longitudine\", round(early_stopper.getBestTrainLosses()[3], 5),  round(early_stopper.getBestValLosses()[3], 5), avgLongitudeTestLoss])\n",
        "models_performance.append([\"MSE Adâncime\", round(early_stopper.getBestTrainLosses()[4], 5),  round(early_stopper.getBestValLosses()[4], 5), avgDepthTestLoss])\n",
        "\n",
        "r2_score_list = r2score_value.cpu().detach().numpy().tolist()\n",
        "models_performance.append([\"R2 Score\", r2_score_list[0], r2_score_list[1], r2_score_list[2], r2_score_list[3]])\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Costurile obtinute pe loturile de antrenare, validare si testare\")\n",
        "print(tabulate(models_performance[0:5], headers=[\"Metrică\", \"Antrenare\", \"Validare\", \"Testare\"], tablefmt=\"github\"))\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"Precizia modelului pe setul de test\")\n",
        "print(tabulate([models_performance[5]], headers=[\"Metrică\", \"Magnitudine\", \"Latitudine\", \"Longitudine\", \"Adâncime\"], tablefmt=\"github\"))\n",
        "\n",
        "# serialize the model to disk\n",
        "# torch.save(model, f\"{models_dir}/ResNet_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhBh9RVGDpKs"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# definesc datele pentru valorile reale (gt) si cele estimate (pred)\n",
        "gt_magnitudes = test_true[:,0]\n",
        "gt_latitudes = test_true[:,1]\n",
        "gt_longitudes = test_true[:,2]\n",
        "gt_depth = test_true[:,3]\n",
        "\n",
        "pred_magnitudes = test_pred[:,0]\n",
        "pred_latitudes = test_pred[:,1]\n",
        "pred_longitudes = test_pred[:,2]\n",
        "pred_depth = test_pred[:,3]\n",
        "\n",
        "# definesc aspectul hartii\n",
        "layout = go.Layout(\n",
        "    mapbox=dict(\n",
        "        center=dict(lat=33.5, lon=-116.8),\n",
        "        zoom=8,\n",
        "        style='stamen-terrain'\n",
        "    ),\n",
        "    title='Epicentrele cutremurelor observate și estimate')\n",
        "\n",
        "# definesc textul afisat la vizualizare punctelor\n",
        "gt_hover_text = ['Numar cutremur: {} <br>Latitudine: {}<br>Longitudine: {} <br>Magnitudine: {} <br>Adancime: {}'.format(num, lat, lon, magn, depth)\n",
        "                 for num, (lat, lon, magn, depth) in enumerate(zip(gt_latitudes, gt_longitudes, gt_magnitudes, gt_depth))]\n",
        "pred_hover_text = ['Numar cutremur: {} <br>Latitudine: {}<br>Longitudine: {} <br>Magnitudine: {} <br>Adancime: {}'.format(num, lat, lon, magn, depth)\n",
        "                   for num, (lat, lon, magn, depth) in enumerate(zip(pred_latitudes, pred_longitudes, pred_magnitudes, pred_depth))]\n",
        "\n",
        "# creez cutiile hartii de dispersie a coord. epicentrelor reale si estim.\n",
        "gt_trace = go.Scattermapbox(\n",
        "    lat=gt_latitudes,\n",
        "    lon=gt_longitudes,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color='blue'\n",
        "    ),\n",
        "    name='Date seismice observate',\n",
        "    hovertext=gt_hover_text,\n",
        "    hoverinfo='text'\n",
        ")\n",
        "\n",
        "pred_trace = go.Scattermapbox(\n",
        "    lat=pred_latitudes,\n",
        "    lon=pred_longitudes,\n",
        "    mode='markers',\n",
        "    marker=dict(\n",
        "        color='red'\n",
        "    ),\n",
        "    name='Date seismice estimate',\n",
        "    hovertext=pred_hover_text,\n",
        "    hoverinfo='text'\n",
        ")\n",
        "\n",
        "# creez figura, adaug punctele si aspectul\n",
        "fig = go.Figure(data=[gt_trace, pred_trace], layout=layout)\n",
        "\n",
        "# salvez harta ca fisier html\n",
        "fig.write_html(f\"{maps_dir}/Harta_ResNet3.html\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}